---
title: 神经网络调参技巧
date: 2024-11-14 10:00:00
tags: 深度学习
---

## 调参顺序

第一个要进行调整的参数就是学习率，需要设置一个好的初始学习率，它可以决定损失函数的降低速度以及损失函数的最低位置。通常从 0.01 开始，**范围从 [1e-6,1e-1]**，通常只选择 1e<sup>n</sup> 。

学习率优化器是针对每个参数进行调整的，学习率调度器是对全局的学习率进行调整的。

| **学习率调度器** | **学习率优化器** | **组合优势** | **适用场景** |
| --- | --- | --- | --- |
| **StepLR** | SGD, Adam | 定期减小学习率，简单高效，防止过拟合 | 大多数监督学习任务，如分类、回归任务 |
| **ReduceLROnPlateau** | SGD, Adam | 自适应调节学习率，基于验证损失自动减少，避免学习率过小 | 验证集性能停滞的任务，如回归、复杂的模型训练 |
| **ExponentialLR** | SGD, Adam, RMSprop | 指数衰减学习率，快速收敛，减少训练时间 | 快速收敛场景，适合较大规模数据集 |
| **CosineAnnealingLR** | SGD, Adam | 平滑衰减学习率，避免震荡，适合长时间训练 | 长期训练的大型深度神经网络，如卷积神经网络 |
| **CyclicLR** | Adam, RMSprop | 周期性调整学习率，跳出局部最优，稳定优化 | 复杂优化任务，如生成对抗网络（GAN）、深度强化学习 |
| **OneCycleLR** | Adam, SGD | 先增大学习率，后逐渐减小，快速找到最优解 | 快速训练大规模神经网络，如图像分类、NLP大模型 |
| **LambdaLR** | 任意优化器 | 完全自定义学习率变化策略，灵活可控 | 特定需求场景，无法用其他调度器满足的情况 |
| **MultiStepLR** | SGD, Adam | 在特定里程碑时减少学习率，适合多阶段训练 | 长期训练的任务，如卷积神经网络、阶段性改进模型性能 |




## 过拟合缓解措施
| **方法** | **原理** | **适用场景** | **优势** | **使用场景** |
| --- | --- | --- | --- | --- |
| **L2 正则化** | 在损失函数中加入权重的平方和作为惩罚项，防止权重过大 | 大多数神经网络，尤其是复杂的深度模型 | 减少权重过大，提升模型的泛化能力 | 数据量大、模型复杂，可能出现过拟合的场景 |
| **L1 正则化** | 在损失函数中加入权重的绝对值和作为惩罚项，倾向于使部分权重趋近于零 | 稀疏模型、特征选择 | 实现特征选择，简化模型 | 需要稀疏性或特征选择的任务，如文本分类 |
| **Dropout** | 训练时随机丢弃部分神经元，减少神经元间共适应性 | 深度神经网络，尤其是全连接层较多的模型 | 提升泛化能力，减少共适应性 | 卷积神经网络（CNN）、循环神经网络（RNN） |
| **早停法（Early Stopping）** | 监控验证集性能，当性能不再提升时停止训练 | 所有深度学习任务，特别是长时间训练的任务 | 节省训练时间，防止过拟合 | 长时间训练的任务，如大型神经网络、语言模型 |
| **数据增强** | 通过随机变换训练数据（如翻转、裁剪等）增加数据的多样性 | 图像分类、时间序列处理等 | 增强数据集多样性，提高模型泛化能力 | 图像分类（翻转、旋转）、语音处理（噪声增强） |
| **Batch Normalization** | 对每层的激活值进行标准化处理，使输入具有稳定的分布 | 深层神经网络，如卷积神经网络和循环神经网络 | 加速收敛，防止过拟合，训练更稳定 | 深层模型（如ResNet、VGG等），用于加速训练和稳定 |
| **减少模型复杂度** | 减少模型的参数量（如降低网络层数、神经元数） | 小数据集、简单任务 | 通过简化模型结构减少过拟合风险 | 数据较少、简单任务，如小型图像分类任务 |
| **交叉验证** | 将数据划分为多个子集，循环验证模型性能 | 小数据集、模型调参 | 多次训练提高模型评估的稳定性 | 小数据集，模型调优，避免单次训练评估误差的场景 |
| **添加噪声** | 在输入、隐藏层或输出层添加噪声，增加模型的训练难度 | 时间序列、音频处理任务 | 增强模型鲁棒性，防止过拟合 | 语音、时间序列等，模型需要适应噪声的场景 |
| **调节批次大小** | 使用较小的批次大小，增加训练中的随机性 | 深度学习模型，特别是CNN和RNN模型 | 小批次能有效提升模型的泛化能力 | 大规模数据处理，小批次可能提升泛化效果的场景 |
| **正则化项** | 在损失函数中添加特定任务的自定义正则化项 | 特定任务，如有特定约束的模型 | 更好地控制模型的复杂性 | 有明确的任务约束，如控制输出范围的场景 |

