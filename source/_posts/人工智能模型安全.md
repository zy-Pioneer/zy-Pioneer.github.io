---
title: 人工智能模型安全
date: 2024-12-14
updated: 2024-12-14
tags: [安全,人工智能]
categories: [信息系统安全课程复习]
---

### 攻击类型：
1. 对抗攻击：旨在通过⼲扰模型预测结果以获取⾮法利益。

  2.  后⻔攻击：通过在模型中植⼊后⻔来破坏模型的可靠性。

3. 隐私攻击：旨在窃取模型或数据中的隐私信息。

##  对抗样本攻击概述
### 对抗样本的概念：
使⽤特定技术对输⼊样本进⾏微⼩的修改就可骗过模型⽽得到错误的结果，这种经过修改，使得模型判断错误的样本被称为对抗样本。

### 威胁模型
1. 黑盒攻击  
攻击者只能给定输⼊去获得模型输出，但并不知道被攻击模型所使⽤的算法和参数，⿊盒攻击可以针对任何⼀个⼈⼯智能模型（可以攻击任何一个人工智能模型）。
2. 白盒攻击  
攻击者熟知⼈⼯智能模型的算法和模型参数，⽣成对抗样本的过程可以与模型的每⼀部分进⾏交互。

### 白盒模型下对抗样本的生成与防御
1. （攻击角度，白盒攻击）对抗样本的生成：对⼈⼯智能模型的⽩盒攻击通常会对模型的每⼀部分进⾏逐层分解，然后对每⼀部分添加⼀定的扰动，使得模型的结果逐步向误判⽬标类别偏移
2. （防御角度）生成对抗网络 GAN：  
由于白盒攻击时对模型内部增添扰动实现的，所以在训练的时候使用同样的方法增强模型训练的鲁棒性，例如对抗生成网络（GAN）

####  GAN的组成：
1. 生成网络：通过神经⽹络将输⼊的⼀个服从简单分布的随机变量（随机噪声）转化为能够欺骗判别⽹络的对抗样本
2. 对抗网络：通过神经⽹络判断输⼊样本的真实类别

在模型训练时，⽣成⽹络负责⽣成对抗样本，判别⽹络（即我们真正需要的⽹络）对样本类别进⾏判断。在这⼀过程中，⽣成⽹络所⽣成的试图欺骗判别⽹络的对抗样本会被判别⽹络识破，从⽽达到防御⽩盒攻击的⽬的。

#### GAN 的训练过程
### 黑盒模型下的攻击与防御
1. （攻击角度，黑盒攻击）在⿊盒威胁模型下，攻击者的⽬标是设计有效的攻击⽅法，使模型在未知攻击下表现出性能下降或错误输出，例如通过设计针对特定模型的对抗样本进⾏攻击
2. （防御角度）针对⿊盒威胁模型的防御策略通常需要设计更加健壮的防御⽅法，例如通过增加数据的多样性和噪声来提⾼模型的鲁棒性，或者通过模型结构的改进和安全性增强来提⾼模型抵抗对抗样本攻击的能⼒。

#### 攻击技术
⼈⼯智能安全威胁的⿊盒攻击技术主要包括对抗性输⼊、数据中毒攻击、反馈武器化和模型窃取技术。

1. 对抗性输⼊：是⼀种专⻔设计的输⼊，旨在确保被误分类，以躲避检测。这种攻击涉及到创建恶意⽂档和试图逃避垃圾邮件过滤器的电⼦邮件等，⽬的是通过欺骗⼈⼯智能系统，使其做出错误的分类或决策。
2. 数据中毒攻击： 数据中毒攻击通过对训练数据进行污染（例如通过添加恶意数据）来影响模型的训练过程，使得模型在推理阶段产生错误的预测。这种攻击方法通常依赖于攻击者能够影响训练数据的获取，但又不能直接访问模型的训练过程，包括
+ **标签污染**：攻击者可以通过向训练数据中添加带有错误标签的数据来误导训练过程，使得模型在训练时产生偏差。
+ **特征污染**：攻击者可以改变输入特征，制造模型在推理时的误判。
3. 模型窃取攻击： 模型提取攻击是攻击者通过查询目标模型并记录其输出，从而反向构建目标模型的近似副本。在黑盒环境下，攻击者并不知道目标模型的结构和参数，但可以通过查询和反馈来模拟出一个接近的模型。构建出的副本模型可以被用来进一步发起对抗攻击或直接被用作攻击其他系统。  
4. 反馈武器化攻击：本质上来说它是一种策略，通过不断观察目标系统的反馈结果如模型输出等，来不断调整输入，来达到攻击目的。
5. 边界攻击：边界攻击通过微小的扰动对输入样本进行调整，迫使目标模型输出错误的结果。攻击者在黑盒环境下通过推测模型的决策边界（decision boundary），逐渐将样本推向这个决策边界，从而逼迫模型做出错误分类。
6. 模型压缩与反向工程： 攻击者可能试图通过**模型压缩**技术将目标模型转换为一个简单的模型，从而揭示其潜在结构和行为。压缩通常是指将一个大规模的神经网络转换成一个更小的网络，通过在保持较高准确度的同时，减少模型的复杂性。 如网络剪枝（减少一些权重）或知识蒸馏（训练替代的小模型）

#### 对抗样本的生成
1. 无针对攻击：任意生成的输入数据，使得模型输出为指定结果（噪声-->模型输出 3）

![](https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/b0a7c540.png)

2. 有针对攻击：生成人类与模型判断迥异的对抗样本（人类认为 9-->模型输出 3），两阶段损失函数，分别保证被人误判和被模型误判

![](https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/48926d0d.png)

#### 防御技术
数据压缩：通过对输⼊数据进⾏压缩或者降维，在保证识别准确率的情况下 提升模型对⼲扰攻击的鲁棒性

数据随机化：对训练数据进⾏随机缩放、增强等操作，提升模型的鲁棒性

训练额外的⽹络来判断训练数据是否为攻击样本

## 人工智能模型安全防护措施
### 攻击类型
1. 对抗攻击：对抗攻击的⽬的在于通过制造并注⼊对抗样本，降低模型的预测准确性。这类攻击通常针对模型的输⼊数据，通过添加⼈类难以察觉的⼲扰，使得模型产⽣错误的输出。
2. 后门攻击：后⻔攻击的⽬的在于在模型训练过程中植⼊后⻔，使得模型在特定情况下产⽣错误的输出。这类攻击通常通过修改模型参数或使⽤特定的训练数据来实现。
3. 隐私攻击：隐私攻击的⽬的在于通过获取模型的隐私信息，如训练数据、模型结构等，来提⾼攻击者的能⼒。这类攻击通常针对模型的所有者，通过窃取或购买的⽅式获取敏感信息。

### 防御类型
1. 检测数据攻击：通过检测数据是否被篡改或伪造来防⽌数据攻击。这可以通过检查数据的完整性、⼀致性或可信度来实现。
2. 检测模型攻击：通过检测模型是否被篡改或⼲扰来防⽌模型攻击。这可以通过检查模型的预测结果、性能或结构来实现。
3. 检测后⻔攻击：通过检测模型是否包含后⻔来防⽌后⻔攻击。这可以通过检查模型的参数、训练数据或输⼊数据来实现。

#### 数据保护
1. 保护数据隐私：通过保护数据隐私来防⽌数据被泄露或滥⽤。这可以通过加密数据、保护数据隐私空间、限制数据访问等⽅式实现。
2. 保护数据安全：通过保护数据安全来防⽌数据被篡改或损坏。这可以通过确保数据的完整性、⼀致性或可信度来实现。
3. 保护数据可⽤：通过保护数据可⽤性来防⽌数据被删除或不可⽤。这可以通过备份数据、保证数据完整性、⼀致性等⽅式实现。

#### 模型增强
1. 增强模型安全性：通过增强模型安全性来防⽌模型被攻击或篡改。这可以通过增加模型参数、使⽤更安全的学习算法、增加模型复杂性等⽅式实现。
2. 增强模型鲁棒性：通过增强模型鲁棒性来提⾼模型对噪声、⼲扰等攻击的抵抗⼒。这可以通过训练模型对噪声和⼲扰的鲁棒性、使⽤更稳定的计算器等实现。
3. 增强模型可解释性：通过增强模型可解释性来提⾼模型决策过程的可理解和可信任度。这可以通过使⽤可解释性强的学习算法、对模型进⾏可视化等实现。

## 人工智能模型安全在信息系统安全中的应用
