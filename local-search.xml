<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2024/11/14/hello-world/"/>
    <url>/2024/11/14/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>单机多卡训练框架</title>
    <link href="/2024/11/14/%E5%8D%95%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E6%A1%86%E6%9E%B6/"/>
    <url>/2024/11/14/%E5%8D%95%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E6%A1%86%E6%9E%B6/</url>
    
    <content type="html"><![CDATA[<p><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;DDP&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">支持在多个机器中进行模型训练，其中每个机器被称之为节点</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;Node&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">，每个机器上有可能有多个</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;GPU&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">，为了不受</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;GIL&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">的限制，</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;DDP&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">会针对每个</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;GPU&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">启动一个进程进行训练，每个进程在对应机器上的编号使用环境变量</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;LOCAL_RANK&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">进行标识。</font></p><p><font style="color:rgb(44, 62, 80);">一次训练，在所有</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;Node&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">上启动的训练进程总和使用</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;WORLD_SIZE&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">来统计，</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;RANK&lt;/font&gt;</code><font style="color:rgb(71, 101, 130);">则表示全局编号（多台机器）。</font></p><p><img src="https://cdn.nlark.com/yuque/0/2024/png/46744832/1722823375269-ff80d19e-8d9e-4834-8490-05ca6f65940f.png"></p><h2 id="1-代码运行"><a href="#1-代码运行" class="headerlink" title="1.代码运行"></a>1.代码运行</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">CUDA_VISIBLE_DEVICES=0,2,3 python -m torch.distributed.launch --nproc_per_node=3 --nnodes=1 train.py<br></code></pre></td></tr></table></figure><p>这里的–nproc_per_node&#x3D;2表示用几张显卡，–nnodes&#x3D;1表示用几台机器</p><p>这里就会根据GPU数量来创建进程分别去执行程序</p><h2 id="2-初始化"><a href="#2-初始化" class="headerlink" title="2.初始化"></a>2.初始化</h2><p><font style="color:rgb(44, 62, 80);">在编写多</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;GPU&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">训练的代码时，需要先对环境进行初始化，需要调用</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;init_process_group&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">来初始化默认的分布式进程组(</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;default distributed process group&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">)和分布式包(</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;distributed package&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">)。使用的是</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;pytorch&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">的</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;torch.distributed.init_process_group&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">方法。</font></p><p><font style="color:rgb(44, 62, 80);">该方法原型：</font></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.distributed.init_process_group(backend=<span class="hljs-literal">None</span>, \<br>                                     init_method=<span class="hljs-literal">None</span>, \<br>                                     timeout=datetime.timedelta(seconds=<span class="hljs-number">1800</span>), \<br>                                     world_size=-<span class="hljs-number">1</span>, \<br>                                     rank=-<span class="hljs-number">1</span>, \<br>                                     store=<span class="hljs-literal">None</span>, \<br>                                     group_name=<span class="hljs-string">&#x27;&#x27;</span>, \<br>                                     pg_options=<span class="hljs-literal">None</span>)<br></code></pre></td></tr></table></figure><p><font style="color:rgb(44, 62, 80);">函数参数：</font></p><ul><li><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;backend&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">: 参数类型为</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;str or Backend&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">，根据</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;pytorch&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">编译时的配置来选择，支持</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;mpi/gloo/nccl/ucc&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">，这个后端指的是多</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;GPU&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">之间进行通信的方式，根据不同类型的</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;GPU&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">进行选择，对于</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;NVIDIA&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">的</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;GPU&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">一般选择</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;nccl&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">，对于</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;Intel&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">的</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;GPU&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">一般选择</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;ucc&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">。</font></li><li><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;init_method&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">: 参数类型为</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;str&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">，指定初始化方法，一般使用</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;env://&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">，表示使用环境变量</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;MASTER_ADDR&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">和</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;MASTER_PORT&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">来初始化。和</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;store&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">变量是互斥的。</font></li><li><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;timeout&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">: 参数类型为</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;datetime.timedelta&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">，指定初始化超时时间，如果超时则抛出异常。</font></li><li><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;world_size&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">: 参数类型为</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;int&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">，指定进程组的大小，如果为</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;-1&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">，则使用环境变量</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;WORLD_SIZE&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">来指定，定义</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;store&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">变量时必须指定</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;world_size&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">。</font></li><li><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;rank&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">: 参数类型为</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;int&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">，指定当前进程在进程组中的排位，如果为</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;-1&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">，则使用环境变量</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;RANK&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">来指定，定义</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;store&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">变量时，必须指定</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;rank&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">。</font></li><li><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;store&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">: 参数类型为</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;Store&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">，指定用于保存分布式训练状态的存储</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;Key/Value&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">对象,用于交换连接&#x2F;地址信息，所有的进程都能访问，和</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;init_method&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">方法互斥。</font></li><li><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;group_name&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">: 参数类型为</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;str&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">，指定进程组的名字，这个变量已经是</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;deprecated&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">了。</font></li><li><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;pg_options&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">: 参数类型为</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;ProcessGroupOptions&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">，指定进程组的其他选项，如</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;allreduce_post_hook&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">等,目前仅对</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;nccl&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">后端支持</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;ProcessGroupNCCL.Options&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">选项。</font></li></ul><p><strong><font style="color:rgb(44, 62, 80);">使用</font></strong><code>**&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;torch.distributed.init_process_group&lt;/font&gt;**</code><strong><font style="color:rgb(44, 62, 80);">初始化进程组的两种方式</font></strong><font style="color:rgb(44, 62, 80);">：</font></p><ul><li><font style="color:rgb(44, 62, 80);">指定</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;store/rank/world_size&lt;/font&gt;</code></li><li><font style="color:rgb(44, 62, 80);">指定</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;init_method&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">，明确给出进程间在哪通过哪种协议发现其他进程并通信，此时</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;rank/world_size&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">是可选的</font></li></ul><p><strong><font style="color:rgb(44, 62, 80);">初始化后，进程组可以通过</font></strong><code>**&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;torch.distributed.get_world_size()&lt;/font&gt;**</code><strong><font style="color:rgb(44, 62, 80);">和</font></strong><code>**&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;torch.distributed.get_rank()&lt;/font&gt;**</code><strong><font style="color:rgb(44, 62, 80);">来获取进程组大小和当前进程在进程组中的排位</font></strong><font style="color:rgb(44, 62, 80);">。</font></p><p><font style="color:rgb(44, 62, 80);">所以最简单的初始化方式，只需要指定后端即可：</font></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.distributed.init_process_group(backend=<span class="hljs-string">&#x27;nccl&#x27;</span>)<br></code></pre></td></tr></table></figure><p><font style="color:rgb(44, 62, 80);">每个进程的环境变量</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;RANK&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">是在启动时由</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;torchrun&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">命令行工具自动添加的，</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;WORLD_SIZE&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">是在</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;torchrun&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">启动时根据启动的进程数自动添加的。</font></p><h2 id="3-数据准备"><a href="#3-数据准备" class="headerlink" title="3.数据准备"></a><font style="color:rgb(44, 62, 80);">3.数据准备</font></h2><p><font style="color:rgb(44, 62, 80);">在使用</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;DistributedDataParallel&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">实现训练时，在数据加载器中上需要使用两个采样器</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;sampler = DistributedSampler(data)&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">和</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;batch_sampler = torch.utils.data.BatchSampler(train_sampler, batch_size, drop_last=True)&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">来指定数据采样器，这样可以保证每个进程每个</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;batch&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">只处理属于自己的数据。</font></p><p><font style="color:rgb(44, 62, 80);">这里一起来看下</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;DistributedSampler&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">和</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;BatchSampler&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">。</font></p><p><code>**&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;DDP&lt;/font&gt;**</code><strong><font style="color:rgb(44, 62, 80);">模式就是将数据均分到多个</font></strong><code>**&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;GPU&lt;/font&gt;**</code><strong><font style="color:rgb(44, 62, 80);">上来优化算法，对于每个</font></strong><code>**&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;GPU&lt;/font&gt;**</code><strong><font style="color:rgb(44, 62, 80);">该如何从总的训练数据中采样属于自己用的数据，这就需要一个采样策略，这正是</font></strong><code>**&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;DistributedSampler&lt;/font&gt;**</code><strong><font style="color:rgb(44, 62, 80);">发挥的作用</font></strong><font style="color:rgb(44, 62, 80);">。</font></p><p><img src="https://cdn.nlark.com/yuque/0/2024/png/46744832/1722823838272-a488c215-2ec4-4fa0-829a-58da502f710f.png"></p><p><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;torch.utils.data.BatchSampler&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">则是指定每个</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;batch&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">的样本数量，以及是否丢弃最后一个可能不足的</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;batch&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">。当设置</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;drop_last=True&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">时，会将最后不足一个</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;batch&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">的数据丢弃。</font></p><p><font style="color:rgb(44, 62, 80);"></font></p><p><font style="color:rgb(44, 62, 80);">上面介绍的过程是对于一轮数据训练时数据加载器的工作过程，对整个训练过程，为了保证学习的效果，需要在每个</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;epoch&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">设置采样器能重新打散数据，因此要在每一轮训练开始前调用</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;DistributedSampler&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">的</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;set_epoch&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">方法。</font></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">sampler = DistributedSampler(data)<br>batch_sampler = torch.utils.data.BatchSampler(<br>        sampler, batch_size, drop_last=<span class="hljs-literal">True</span>)<br>dataloader = torch.utils.data.Dataloader(data_set, batch_sampler=train_batch_sampler)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoches):<br>    sampler.set_epoch(epoch)<br>    ...<br></code></pre></td></tr></table></figure><h2 id="4-模型准备"><a href="#4-模型准备" class="headerlink" title="4.模型准备"></a>4.模型准备</h2><p><font style="color:rgb(44, 62, 80);">使用</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;DistributedDataParallel&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">进行模型训练时，需要将模型放在</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;DistributedDataParallel&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">类中，这样模型就可以在多</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;GPU&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">上并行计算。</font></p><p><font style="color:rgb(44, 62, 80);">此外，还有一些需要注意的。</font></p><p><font style="color:rgb(44, 62, 80);">在设置</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;device&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">时，要想指定使用的</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;GPU&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">需要设置环境变量</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;CUDA_VISIBLE_DEVICES=1,2&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">，在代码中对于模型，可以使用</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;model.to(device)来设置&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">，</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;device&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">从</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;LOCAL_RANK&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">中获取，当设置</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;CUDA_VISIBLE_DEVICES&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">时，</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;LOCAL_RANK&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">的</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;0&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">时从指定的</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;GPU&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">开始的，而不是硬件上的</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;GPU&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">序号，例如指定</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;CUDA_VISIBLE_DEVICES=1,2&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">时，</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;LOCAL_RANK=0&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">时对应的是</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;GPU1&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">，</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;LOCAL_RANK=1&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">时对应的是</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;GPU2&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">。</font></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># in train.py</span><br><span class="hljs-keyword">import</span> os<br>device = <span class="hljs-string">f&#x27;cuda:<span class="hljs-subst">&#123;os.getenv(LOCAL_RANK)&#125;</span>&#x27;</span><br></code></pre></td></tr></table></figure><p><font style="color:rgb(44, 62, 80);">加载模型后对于使用多</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;GPU&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">时还需注意的是参数初始化，要使用同一份权重值对模型进行初始化，否则在模型训练时，每个</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;GPU&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">上的模型参数就会不一样，从而导致训练效果不佳。一种方案是将主进程上的权重先保存下来，然后再加载到其他进程的模型上：</font></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">model = Model()<br>ckpt_path = <span class="hljs-string">&quot;/tmp/init_weight.pt&quot;</span><br><span class="hljs-keyword">if</span> <span class="hljs-built_in">int</span>(os.getenv(<span class="hljs-string">&quot;LOCAL_RANK&quot;</span>)) == <span class="hljs-number">0</span>:<br>    torch.save(model.state_dict(), ckpt_path)<br>torch.distributed.barrier()<br>model.load_state_dict(torch.load(ckpt_path, map_location=device))<br>model = DDP(model, device_ids=[local_rank])<br></code></pre></td></tr></table></figure><p><font style="color:rgb(44, 62, 80);">上面的代码的功能很明了，值得注意的是</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;dist.barrier()&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">语句，它表示等待所有进程都到达这个语句处，然后才进行下一步操作，确保所有进程都执行到这一步，然后才开始进行权重加载。</font></p><p><font style="color:rgb(44, 62, 80);">当模型使用</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;BatchNormalization&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">时，除了需要将模型放入</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;DistributedDataParallel&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">类中，还需要使用</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;torch.nn.SyncBatchNorm.convert_sync_batchnorm&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">方法对模型上的</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;BN&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">层进行转化，这样模型训练时，每个</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;GPU&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">上的</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;BatchNormalization&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">层就会与其他</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;GPU&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">上的</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;BN&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">层进行同步更新。</font></p><p><font style="color:rgb(44, 62, 80);">到这里，能够在多</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;GPU&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">上训练的模型就准备好了。下面再来看下模型训练时需要留意的地方。</font></p><ul><li><strong><font style="color:rgb(44, 62, 80);">训练过程中平均损失值的计算</font></strong><font style="color:rgb(44, 62, 80);">。在单个进程中</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;loss&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">是在单个进程数据上计算的，为了记录训练过程，打印平均损失值时，要将所有进程上的</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;loss&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">值累加后除以进程组的大小，以得到平均损失值。</font></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">reduce_value</span>(<span class="hljs-params">value, average=<span class="hljs-literal">True</span></span>):<br>    world_size = get_world_size()<br>    <span class="hljs-keyword">if</span> world_size &lt; <span class="hljs-number">2</span>:  <span class="hljs-comment"># 单GPU的情况</span><br>        <span class="hljs-keyword">return</span> value<br><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        dist.all_reduce(value)<br>        <span class="hljs-keyword">if</span> average:<br>            value /= world_size<br><br>        <span class="hljs-keyword">return</span> value<br><br>reduace_value(loss)<br></code></pre></td></tr></table></figure><p><font style="color:rgb(44, 62, 80);">注意上面代码中使用的</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;dist.all_reduce&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">函数，它用于进行数据同步，将数据从所有进程收集到主进程上，并将主进程上的数据广播到所有进程上，这样所有进程上的数据就相同了。</font></p><ul><li><font style="color:rgb(44, 62, 80);">训练完一个</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;epoch&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">时，在每个进程中要使用</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;torch.cuda.synchronize(device)&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">，以确保使用当前设备的所有进程都计算完成。</font></li><li><font style="color:rgb(44, 62, 80);">在训练过程中使用</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;DDP&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">模型，进程验证时，也需要使用</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;dist.all_reduce&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">来统计所有的运算结果：</font></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@torch.no_grad()</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate</span>(<span class="hljs-params">model, data_loader, device</span>):<br>    model.<span class="hljs-built_in">eval</span>()<br><br>    sum_num = torch.zeros(<span class="hljs-number">1</span>).to(device)<br><br>    <span class="hljs-comment"># 在进程0中打印验证进度</span><br>    <span class="hljs-keyword">if</span> os.getenv(<span class="hljs-string">&quot;RANK&quot;</span>)==<span class="hljs-number">0</span>:<br>        data_loader = tqdm(data_loader, file=sys.stdout)<br><br>    <span class="hljs-keyword">for</span> step, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(data_loader):<br>        images, labels = data<br>        pred = model(images.to(device))<br>        pred = torch.<span class="hljs-built_in">max</span>(pred, dim=<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>]<br>        sum_num += torch.eq(pred, labels.to(device)).<span class="hljs-built_in">sum</span>()<br><br>    <span class="hljs-comment"># 等待所有进程计算完毕</span><br>    <span class="hljs-keyword">if</span> device != torch.device(<span class="hljs-string">&quot;cpu&quot;</span>):<br>        torch.cuda.synchronize(device)<br><br>    sum_num = dist.all_reduce(sum_num)<br><br>    <span class="hljs-keyword">return</span> sum_num.item()<br></code></pre></td></tr></table></figure><h2 id="5-运行"><a href="#5-运行" class="headerlink" title="5.运行"></a>5.运行</h2><p><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;pytorch DistributedDataParallel&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">多</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;GPU&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">训练任务启动的命令通常使用的是</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;python -m torch.distributed.launch&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">，在</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;torch1.9.0&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">版本后引入了</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;torchrun&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">命令，两者功能基本类似，python -m torch.distributed.launch 和 torchrun 在功能上是类似的。它们都是用于启动分布式训练的命令行工具，可以自动设置环境变量并启动训练脚本。</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;torchrun&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);"> </font><font style="color:rgb(44, 62, 80);">是</font><font style="color:rgb(44, 62, 80);"> </font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;PyTorch 1.9.0 版本引入的新命令，旨在为分布式训练提供更简洁和一致的接口。与&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">python -m torch.distributed.launch</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;相比，&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">torchrun</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;具有一些额外的功能和灵活性，例如支持不同的运行模式和分布式运行时后端。对于使用较新版本&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">PyTorch</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;的情况，建议使用&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">torchrun&#96; 来保持一致性以使用其提供的新功能。</font></p><p><font style="color:rgb(44, 62, 80);">关于</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;torchrun&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">和</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;python -m torch.distributed.launch&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">命令支持的选项可以使用</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;--help&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">来查看。</font></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plain">CUDA_VISIBLE_DEVICES=0,2,3 torchrun --nnodes 1 --nproc_per_node 3 train.py<br><br>python -m torch.distributed.launch --nnodes 1 --nproc_per_node train.py train_args<br></code></pre></td></tr></table></figure><p><font style="color:rgb(44, 62, 80);">更底层的方法可以使用</font><code>&lt;font style=&quot;color:rgb(71, 101, 130);&quot;&gt;torch.multiprocessing.spawn&lt;/font&gt;</code><font style="color:rgb(44, 62, 80);">函数来启动训练，它需要传递一个训练函数和进程数量作为参数。</font></p><p><font style="color:rgb(44, 62, 80);"></font></p><h2 id="6-模型保存与加载"><a href="#6-模型保存与加载" class="headerlink" title="6.模型保存与加载"></a><font style="color:rgb(44, 62, 80);">6.模型保存与加载</font></h2><p><font style="color:rgb(44, 62, 80);">另外，如果使用了DDP包装模型，为了只保存模型参数而不包含DDP信息，那么在保存模型时需要使用：</font></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.save(model.module.state_dict(), model_path)<br></code></pre></td></tr></table></figure><p>在加载模型的时候</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.load_state_dict(torch.load(model_path, map_location=device))<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs plain">def tensor_find(t,x):<br>    t_np=t.cpu().numpy()<br>    idx=np.argwhere(t_np==x)<br>    return idx[0][0]+1<br><br>def add_edge_with_node_attributes(G, u, v, msg, u_type, v_type, timestamp):<br>    num = 0<br>    edge_type = tensor_find(msg[16:-16],1)<br>    if u not in G.nodes:<br>        G.add_node(u, type=u_type, label=False)<br>    if v not in G.nodes:<br>        G.add_node(v, type=v_type, label=False)<br>    # 添加边<br>    G.add_edge(u, v, msg=msg, timestamp=timestamp)<br>    <br>    # # 更新节点 u 的属性<br>    # if u_type is not None:<br>    #     if u in G.nodes:<br>    #         G.nodes[u][&#x27;type&#x27;] = u_type<br>    #     else:<br>    #         G.add_node(u, type=u_type)<br>    <br>    # # 更新节点 v 的属性<br>    # if v_type is not None:<br>    #     if v in G.nodes:<br>    #         G.nodes[v][&#x27;type&#x27;] = v_type<br>    #     else:<br>    #         G.add_node(v, type=v_type)<br>    <br>    if u_type == &#x27;netflow&#x27; or v_type == &#x27;netflow&#x27; or G.nodes[u][&#x27;label&#x27;] or G.nodes[v][&#x27;label&#x27;] and edge_type != 3:<br>        G.nodes[u][&#x27;label&#x27;] = True<br>        G.nodes[v][&#x27;label&#x27;] = True<br>        num = 2<br>    return num<br><br><br></code></pre></td></tr></table></figure><h2 id=""><a href="#" class="headerlink" title=""></a></h2>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
