<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>P2IM:Scalable and Hardware-independent Firmware Testing via Automatic Peripheral Interface Modeling</title>
    <link href="/2025/01/16/P2IM_%20Scalable%20and%20Hardware-independent%20Firmware%20Testing%20via%20Automatic%20Peripheral%20Interface%20Modeling/"/>
    <url>/2025/01/16/P2IM_%20Scalable%20and%20Hardware-independent%20Firmware%20Testing%20via%20Automatic%20Peripheral%20Interface%20Modeling/</url>
    
    <content type="html"><![CDATA[<p>本文章旨在解决嵌入式固件的动态测试或模糊测试由于硬件依赖性和扩展性差所带来的挑战，使得可以进行大规模模糊测试。</p><h3 id="主要内容摘要："><a href="#主要内容摘要：" class="headerlink" title="主要内容摘要："></a>主要内容摘要：</h3><p><strong>研究问题：</strong> 该论文聚焦于嵌入式固件模糊测试的限制，尤其是硬件依赖性问题。传统模糊测试工具虽然在通用计算机系统上表现良好，但由于嵌入式系统中硬件外设的多样化和限制，使得它们难以直接应用于 MCU 固件的测试。</p><p><strong>解决方案：</strong> 作者提出了一个名为 P2IM（处理器-外设接口建模）的框架，用于硬件无关且具有可扩展性的固件测试。P2IM 通过自动建模不同外设的 I&#x2F;O 行为来抽象外设操作，使固件能够在不依赖真实硬件的情况下运行，从而可以在标准模糊测试工具（如 AFL）和处理器仿真器（如 QEMU）中对固件进行测试。</p><h3 id="方法论："><a href="#方法论：" class="headerlink" title="方法论："></a>方法论：</h3><ul><li><strong>处理器-外设接口建模（P2IM）：</strong> 论文的核心创新是 P2IM，它通过抽象外设的行为，使模糊测试工具能够像与真实硬件交互一样与固件交互。框架通过生成外设接口的模型，允许固件通过通用处理程序与仿真外设交互。</li><li><strong>模型实例化：</strong> 框架在运行时自动为特定固件生成抽象模型，并通过“探索性执行”技术识别外设寄存器的访问方式，并根据固件的需要动态调整模型。</li><li><strong>中断管理：</strong> 论文还提出了一种无需真实硬件即可处理中断的新方法，将中断触发抽象为特殊输入，可以自定义定时和行为以模拟硬件的操作。</li></ul><h3 id="P2IM-具体实现："><a href="#P2IM-具体实现：" class="headerlink" title="P2IM 具体实现："></a>P2IM 具体实现：</h3><h4 id="寄存器类型判断"><a href="#寄存器类型判断" class="headerlink" title="寄存器类型判断"></a>寄存器类型判断</h4><p>仿真器将内存映射为寄存器，因此要判断每个内存区域（寄存器）的类型， 通过固件在运行时与寄存器的交互行为来推断：</p><ol><li>监控固件的寄存器访问<br>P2IM会在固件执行过程中，监控固件对特定地址范围（通常是内存映射的外设寄存器区域）的访问行为。这包括：<ul><li>读取操作：固件从某个寄存器中读取数据时，P2IM会记录读取的次数、读取的时机，以及固件是否依赖这些数据做出进一步决策。</li><li>写入操作：固件向某个寄存器写入数据时，P2IM会记录写入的数值、频率，以及这些写入操作是否影响固件的执行流或外设状态。</li></ul></li><li>分类和标记寄存器类型<br>基于监控到的访问行为，P2IM对寄存器进行分类和标记。不同类型的寄存器具有不同的访问模式：<ul><li>控制寄存器：如果固件对某个寄存器经常进行写操作，且写入的值通常是用于配置外设状态（例如启用某个功能或设置操作模式），P2IM会将其标记为控制寄存器。</li><li>状态寄存器：如果固件对某个寄存器频繁进行读取操作，并且这些读取操作用于检查外设的状态（例如是否完成某项操作），则P2IM会将该寄存器标记为状态寄存器。</li><li>数据寄存器：如果固件对某个寄存器进行读写操作且该寄存器用于传递实际的数据（例如传感器测量结果或通信数据），P2IM会将其标记为数据寄存器。</li></ul></li><li>探索性执行进行验证<br>在分类的初步推断过程中，P2IM使用探索性执行技术来验证这些推断是否准确。通过暂停固件的执行，P2IM可以尝试向某个寄存器写入或读取不同的值，然后观察固件的反应：<ul><li>如果固件继续执行并进入正确的操作路径（例如完成了一次外设操作或处理了一个中断），P2IM就可以确认这种访问模式是合理的，并且该寄存器的功能得到了正确推断。</li><li>如果固件进入错误路径或卡死，P2IM会调整其推断并尝试其他可能的访问模式，直到找到一个能让固件正常执行的正确模式。</li></ul></li><li>动态调整模型<br>随着固件的执行，P2IM会不断根据固件的行为动态调整寄存器模型。例如，如果固件在不同的上下文中以不同方式访问相同的寄存器（例如在初始化阶段作为控制寄存器，运行阶段作为状态寄存器），P2IM会根据固件的操作动态调整该寄存器的类型和行为。</li></ol><h4 id="探索性执行"><a href="#探索性执行" class="headerlink" title="探索性执行"></a>探索性执行</h4><p>在判断完成之后，就要确定每次交互时寄存器的值，具体如下：</p><ol><li><strong>暂停固件执行并快照保存状态</strong>：当固件在运行过程中访问状态寄存器（例如轮询等待某个外设状态变化）时，如果P2IM未事先为该寄存器建模，它会暂时暂停固件的执行，并对固件当前的状态进行快照保存。这包括保存当前的寄存器状态、堆栈信息和其他固件运行的上下文，以便在探索性执行完成后能够恢复到这一点继续执行。</li><li><strong>生成候选寄存器值</strong>：在暂停固件后，P2IM会生成一组可能的寄存器值（即<strong>候选值</strong>），每个值代表一种潜在的外设状态。例如，对于一个32位的状态寄存器，P2IM可能会生成32个不同的值，每个值只设置一个位为1，其他位为0，外加一个全为0的初始状态。这些候选值反映了寄存器中不同位的可能状态组合。</li><li><strong>并行执行探索分支</strong>：P2IM会为每一个候选寄存器值创建一个并行的执行分支（即<strong>工作线程</strong>）。在每个分支中，固件会使用这个候选值继续执行。P2IM会监控每个分支的执行进度，跟踪固件是否能够通过内部检查并继续正常运行。<ol><li><strong>并行性</strong>：这些探索分支是并行执行的，每个分支尝试不同的寄存器值，模拟固件在不同外设状态下的行为。</li><li><strong>监控执行结果</strong>：P2IM监控各个分支的执行情况，重点关注固件是否因不合理的状态值而崩溃、卡死，或者无法继续执行。</li></ol></li><li><strong>判断分支的有效性</strong> 在每个分支执行一段时间后，P2IM会评估其有效性。具体来说，P2IM会判断以下几点：<ol><li><strong>固件是否崩溃或卡死</strong>：如果某个候选寄存器值导致固件崩溃或无法继续执行，该分支将被视为无效。</li><li><strong>固件是否通过了内部检查</strong>：如果某个分支的寄存器值使固件成功通过了状态检查并继续运行，该分支将被视为有效。</li></ol></li><li><strong>选择最佳寄存器值</strong> P2IM会根据各个分支的执行结果选择<strong>最佳寄存器值</strong>。通常情况下，最佳值是能够使固件继续执行并触发预期的外设操作的值。P2IM可能会依据以下标准来选择最佳值：<ol><li><strong>固件成功执行的程度</strong>：固件继续执行的路径是否符合预期。</li><li><strong>触发的外设操作</strong>：固件是否成功与外设交互，例如从寄存器中读取数据或者处理中断。</li></ol></li><li><strong>恢复固件执行</strong> 一旦P2IM确定了最佳的寄存器值，它会将该值应用到状态寄存器中，并恢复固件的执行，继续从暂停点运行。固件此时将以被探索性执行选择的值作为外设状态，从而继续其预期的操作。</li><li><strong>动态调整模型</strong> 当探索性执行成功推断出某个寄存器的合理值后，P2IM会将此信息添加到模型中，用于后续的执行。如果固件再次访问该寄存器，P2IM可以直接返回已确定的正确值，而不必重新进行探索性执行。这使得模型随着固件的执行不断完善，越来越准确地模拟外设的行为。</li></ol><h3 id="关键成果："><a href="#关键成果：" class="headerlink" title="关键成果："></a>关键成果：</h3><ul><li><strong>可扩展性：</strong> P2IM 成功在没有人工干预的情况下运行了 70 个固件样本中的 79%，表明该框架具备良好的可扩展性，能够处理各种固件。</li><li><strong>漏洞发现：</strong> 在 10 个真实固件样本上进行的有限模糊测试中，框架发现了 7 个此前未知的漏洞，展示了其在提高固件安全性方面的潜力。</li></ul><h3 id="结论："><a href="#结论：" class="headerlink" title="结论："></a>结论：</h3><p>P2IM 是硬件无关固件测试的显著进步，提供了一个可扩展的解决方案，弥合了传统模糊测试工具与 MCU 固件之间的差距。该方法有望在提高 IoT 设备固件安全性方面发挥重要作用，帮助减少未经过充分测试的固件带来的安全风险。</p>]]></content>
    
    
    <categories>
      
      <category>论文笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>漏洞挖掘</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PROVNINJA论文阅读</title>
    <link href="/2025/01/16/PROVNINJA%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <url>/2025/01/16/PROVNINJA%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<p>该论文提出了一个名为 <strong>PROVNINJA</strong> 的框架，用来设计和生成对抗性攻击，目的是规避基于系统溯源<strong>数据的机器学习检测器。系统溯源用于追踪系统中进程、文件、网络套接字等资源之间的操作和依赖关系，这些信息通过</strong>溯源图来表示。PROVNINJA的主要目标是伪装恶意行为，使它们看起来像正常系统操作，从而规避检测。</p><p><img src="https://cdn.nlark.com/yuque/0/2024/png/46744832/1728363715637-8e45fd21-7754-4328-ba74-b799609e6ec1.png"></p><h2 id="XliVX"> Introduction</h2>基于溯源的ML检测器分析系统中详细的运行时数据，通过检测异常行为来防御潜在的 APT 和无文件恶意软件等复杂攻击。然而，现有的基于溯源的ML检测器并未完全抵御攻击者可能使用的对抗性技术，攻击者可以设计看似正常的系统操作来绕过检测。这种情况下需要一个系统化的方法来生成对抗性攻击，使其既能够完成恶意目标，又能尽量不被检测器发现。<h2 id="I4E44">PROVNINJA的核心框架</h2>PROVNINJA通过三个关键阶段实现对抗性攻击的生成与实施：<h3 id="Gkur5"> 识别显眼事件</h3>显眼事件是指那些容易引起ML检测器注意的系统事件。这些事件可能具有以下特点：<ul><li>在系统中出现频率较低（稀有性高）。</li><li>对ML检测器的预测结果影响大。</li></ul><h4 id="ASPp1"> 频率分析</h4>PROVNINJA通过计算系统事件的**规则性评分（Regularity Score）**来判断事件的稀有程度。规则性评分的公式如下：<p><img src="https://cdn.nlark.com/yuque/0/2024/png/46744832/1728356583881-5460842d-8454-4719-9fcf-005055bd14aa.png"></p><p>通过计算每个事件的规则性评分，PROVNINJA可以识别出那些在系统活动中不常见的显眼事件，这些事件很容易引发检测器的警报。</p><h4 id="cvvfp">图结构分析</h4>1. 事件的传递性影响<p>在系统中，某些事件可能会通过一条长路径传播影响。一个显眼事件往往是那些能引发后续多个操作的事件。例如：</p><ul><li>如果进程A创建了进程B，而进程B执行了网络连接或修改了系统文件，那么进程A的行为会在之后通过进程B的操作对系统产生重大影响。</li><li>如果某个事件（如进程创建、文件读写）位于路径的关键位置，它不仅影响直接相连的节点，还可能通过多个中介节点传递影响到系统的其他部分。</li></ul><p>因此，具有<strong>广泛传递性</strong>的事件通常是显眼事件。攻击者可能希望通过替换这些事件来隐蔽攻击路径。</p><hr><ol start="2"><li>事件在路径中的位置</li></ol><p>在因果路径中，事件的相对位置非常重要。通常，<strong>位于因果路径关键位置</strong>的事件（例如路径的起点或中间位置）具有更高的显眼性，因为这些位置的事件可能直接影响后续事件的执行：</p><ul><li>如果某个事件发生在路径的起点，它通常是整个攻击链的触发点，且对于后续的攻击操作至关重要。</li><li>中间节点的事件（例如创建关键进程或修改重要文件）也可能是显眼的，因为它们连接了路径的不同阶段，起到了承前启后的作用。</li></ul><p>这些位置上的事件如果表现异常，就更容易被ML检测器捕捉到。</p><ol start="3"><li>路径的长度和复杂度</li></ol><p><strong>路径的长度和复杂度</strong>也是判断事件显眼性的一个重要指标。通常，越长的路径包含的操作和依赖关系越多，检测器可能会更加关注这类复杂路径中的关键节点和事件：</p><ul><li>如果某个事件位于一条长路径的中间，它的异常行为可能会被放大，因为它可能影响到路径的多个下游节点。</li><li>同样，如果一个路径包含多个高度依赖的节点或资源，这些资源之间的依赖关系越紧密，路径中的异常事件就越容易引起检测器的警觉。</li></ul><ol start="4"><li>节点和边的重要性（图结构分析）</li></ol><p>在因果路径分析中，除了事件本身的位置和传递性，还可以通过图的结构特性来分析事件的重要性。通过分析图中的<strong>节点和边的中心性</strong>，可以判断某个节点（事件）是否在整个系统图中起到关键作用。例如：</p><ul><li><strong>介数中心性（Betweenness Centrality）</strong>：介数中心性衡量一个节点在多条最短路径中的重要性。如果一个节点具有较高的介数中心性，说明该节点位于系统中多条重要路径的中间，连接着大量其他节点。这样的节点通常是显眼事件，因为它们控制着多个系统资源之间的交互。</li><li><strong>度数中心性（Degree Centrality）</strong>：一个节点的度数表示它与其他节点相连的边数。如果一个节点的度数很高（即它连接了许多其他节点），那么该节点可能是一个系统中高度活跃的资源。如果该节点的行为在系统中不常见，它可能成为显眼事件，因为它与系统中的多个资源有关联，且容易影响其他节点的行为。</li></ul><p>这些图结构分析可以帮助确定哪些事件在系统行为中起到至关重要的作用，从而可能引起检测器的重点关注。</p><h3 id="gL0XC">特征空间规避</h3>在特征空间规避中，PROVNINJA利用所谓的“**Gadget链**”来替换显眼事件。Gadget链是由一系列规则性较高的系统事件构成的，它们在系统中更加常见，因此替换后不会显得异常。<p>Gadget链的生成过程包括以下步骤：</p><ol><li><strong>寻找显眼事件的替代事件</strong>：PROVNINJA从“替代库”中寻找能够替代显眼事件的其他系统操作。替代事件必须符合两个条件：<ul><li>保持攻击者的目标不变。例如，如果攻击者需要创建一个恶意进程，替代事件也应能实现相似的操作。</li><li>替代事件应尽可能模仿正常的系统行为，避免引起检测器的警觉。</li></ul></li><li><strong>生成Gadget链</strong>：有时，单个替代事件不足以完成复杂的攻击任务。PROVNINJA可以通过多个Gadget链的组合来替换显眼事件，这些链条连接了一系列常见的系统操作，从而有效地掩盖了恶意行为。</li><li><strong>优化攻击路径</strong>：通过将显眼事件替换为更常见的事件，攻击路径在ML检测器看来更加像正常的系统活动。这样可以显著降低检测器对攻击的检测概率。</li></ol><p>在进行替换时，攻击者通过将显眼事件替换、攻击路径分散和路径加长（广度和深度）这些手段来混淆检测模型。</p><h3 id="hCY3Z">问题空间实现</h3>这是将特征空间中的规避策略转换为实际系统行为的过程，即将Gadget链应用于真实系统。在这个阶段，PROVNINJA的主要挑战是如何将理论上的规避方法应用到现实的复杂系统环境中。<p><strong>问题空间的实现面临以下几大挑战</strong>：</p><ol><li><strong>保持攻击语义</strong>：在进行攻击时，替代事件必须能够实现与原始显眼事件相同的恶意目标。例如，如果原本的恶意事件需要创建某个进程或修改某个文件，替代事件也必须具备类似功能，否则攻击目标将无法达成。</li><li><strong>处理系统环境差异</strong>：不同系统之间的配置和依赖关系可能会影响攻击实施。例如，一个系统在某个时间点的状态可能与在其他时间点有所不同，或系统运行的环境与攻击者的测试环境不一致，这些都可能影响攻击的实施。</li><li><strong>事件的可实现性</strong>：某些替代事件或Gadget链在攻击者的测试环境中可能可行，但在目标系统中却可能因为权限不足、系统配置差异等问题而无法实际实现。因此，PROVNINJA需要通过实际的系统环境进行校验，确保攻击路径的可执行性。</li></ol><h2 id="vRqkN">威胁模型</h2>PROVNINJA框架被设计用于不同类型的威胁模型，其中主要包括三种类型：<ul><li><strong>黑箱模型</strong>：攻击者无法直接访问或查询ML检测器的内部结构，但可以通过一些外部查询（如查询公开的程序执行频率）来推断系统行为。PROVNINJA在黑箱模型下主要依赖于从公开数据集中收集的行为数据，生成规避攻击。</li><li><strong>白箱模型</strong>：攻击者对ML检测器的内部工作机制有完全了解，包括模型的架构和参数。在这种情况下，PROVNINJA可以通过工具（如GNNExplainer）来识别哪些事件对ML检测器的决策贡献最大，从而设计更有针对性的攻击路径。</li><li><strong>盲箱模型</strong>：攻击者无法查询检测器，也无法获得系统的行为数据，只能基于公共数据集或对系统行为的推断来设计攻击。这是最为困难的模型。</li></ul>]]></content>
    
    
    <categories>
      
      <category>论文笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>安全</tag>
      
      <tag>入侵检测</tag>
      
      <tag>PIDS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>IOT固件仿真测试方法</title>
    <link href="/2025/01/16/%E5%9F%BA%E4%BA%8E%E4%BB%BF%E7%9C%9F%E7%9A%84%E6%B5%8B%E8%AF%95%E6%96%B9%E6%B3%95/"/>
    <url>/2025/01/16/%E5%9F%BA%E4%BA%8E%E4%BB%BF%E7%9C%9F%E7%9A%84%E6%B5%8B%E8%AF%95%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="基于仿真的测试方法及其挑战"><a href="#基于仿真的测试方法及其挑战" class="headerlink" title="基于仿真的测试方法及其挑战"></a>基于仿真的测试方法及其挑战</h2><p>固件仿真是近年来的一项新兴技术。模拟器允许安全研究人员在没有硬件的情况下运行嵌入式软件。大多数动态测试通常都伴随着破坏性。例如，模糊测试会导致设备反复进入崩溃状态。因此，对于脆弱且昂贵的物联网设备，仿真和动态测试是一个有吸引力的解决方案。然而，成功的仿真并不像将固件导入仿真器那么简单。它将面临许多现实世界环境限制带来的挑战。例如，不统一的固件开发原则导致需要手动定制仿真固件所需的仿真器。此外，在模拟器上部署各种测试工具并测试正在运行的固件也具有挑战性。</p><p>进行仿真测试需要解决一下四个问题：</p><ol><li>获取固件；</li><li>外部硬件及外围设备；</li><li>大规模测试；</li><li>检测漏洞。</li></ol><h2 id="固件获取"><a href="#固件获取" class="headerlink" title="固件获取"></a>固件获取</h2><p>固件是整个仿真过程的关键。许多现有方法讨论如何进行仿真，但忽略了固件获取过程。为了保护产品的安全，大多数厂家并没有发布相关的固件。而且，物联网设备的各种调试端口被制造商封锁，以防止固件泄露。因此，获取目标固件具有挑战性。为了克服这一挑战，研究人员提出了以下不同的解决方案：</p><ol><li>互联网上有很多固件的镜像可以下载，但通常都并不完善；</li><li>固件进行更新时会下载最新固件到设备上，可以通过网络流量拦截的方式获取，但通常都会被加密或只是下载补丁而不是完整的固件。</li></ol><h2 id="外部硬件及外围设备"><a href="#外部硬件及外围设备" class="headerlink" title="外部硬件及外围设备"></a>外部硬件及外围设备</h2><p> 由于固件需要与特定的硬件外围设备交互，而这些外围设备在虚拟化器（如QEMU）中可能无法得到充分仿真，因此有些固件无法在虚拟环境中正常仿真。此外，许多设备都定制了自己的固件，导致其体系结构和内核标准不一致，使得仿真变得更加复杂和多样化。  因此分为部分仿真和完全仿真：</p><blockquote><ol><li>部分仿真<br>例子：嵌入式设备的固件分析</li></ol><p>假设你正在分析一个智能家居设备的固件，比如智能灯泡的固件。这个固件依赖于嵌入式硬件（如LED控制芯片、无线通信模块等）来实现其功能。使用部分仿真的方法，你可能只仿真固件的部分功能，比如固件的启动过程、操作系统的初始化或一些核心逻辑（如处理指令或控制灯泡开关的代码）。</p><p>部分仿真的一个常见做法是跳过固件中对特定硬件的调用，这些硬件在虚拟环境中无法被精确仿真。例如，QEMU 可能无法仿真无线通信模块。因此，部分仿真可以忽略或模拟这些外设的存在，而只聚焦于固件的软件部分，从而分析出设备的部分行为。</p><p>优势：这种方法能够快速获得关于固件的一些信息，而不需要精确仿真所有硬件。但它的局限性在于，无法分析固件与硬件的深层次交互，比如无法仿真出固件如何控制LED灯或与Wi-Fi模块通信的行为。</p><ol start="2"><li>完全仿真<br>例子：路由器固件的全仿真</li></ol><p>假设你正在对路由器固件进行安全分析。路由器的固件往往与多个外围设备交互，例如网卡、交换芯片、USB接口等。在这种情况下，完全仿真方法试图精确仿真固件与这些外设的所有交互。</p><p>例如，QEMU 通过仿真ARM架构，可以完整地仿真路由器固件的启动过程、驱动程序加载、网络接口初始化、无线网卡和交换芯片的工作流程等。为实现这一点，仿真环境可能需要定制设备模型，甚至模拟特定硬件的寄存器行为，以便精确还原固件的工作方式。</p><p>优势：完全仿真可以提供更高的精确度，特别适合进行复杂的固件安全分析或逆向工程研究。分析人员可以模拟设备的真实行为，包括固件如何与硬件协作。这有助于发现隐藏在硬件交互中的漏洞或后门，例如网络接口的安全漏洞或USB接口的未授权访问问题。</p></blockquote><h3 id="部分仿真"><a href="#部分仿真" class="headerlink" title="部分仿真"></a>部分仿真</h3><p>在嵌入式设备的漏洞挖掘中，由于外设的复杂性和硬件与软件的紧密结合，完全仿真往往困难重重。因此，部分仿真成为了解决这一问题的重要方法。以下是对 PROSPECT、AVATAR、SURROGATES 和 ECMO 等几种部分仿真方法的详细描述：</p><ol><li>PROSPECT<br>概述：<br>PROSPECT 是一个能够克服仿真外设困难的系统。它的核心思想是将固件对外设的硬件访问透明地从主机系统转发到虚拟机。这种转发机制使得嵌入式软件能够在虚拟环境中运行，而无需了解具体外设的操作细节。</li></ol><p>技术原理：<br>PROSPECT 的部分仿真工作流程如下：</p><p>在仿真器中，固件继续运行，并发出与外设的交互请求。这些请求通常是通过I&#x2F;O指令或存储器映射的外设访问实现的。<br>PROSPECT 将这些外设访问请求捕获并透明地转发到真实的外设硬件。外设硬件响应后，其结果会被返回给虚拟机中的固件，仿真器接收并处理该结果，就好像外设实际上是通过仿真器直接控制的一样。<br>应用场景：<br>PROSPECT 适用于那些固件深度依赖专有外设的场景，特别是复杂的嵌入式设备，如物联网设备或工业控制系统。通过这种透明转发机制，固件能够在不完全仿真外设的情况下运行，并保持其预期的功能行为。</p><ol start="2"><li>AVATAR<br>概述：<br>AVATAR 是一个仿真框架，它允许仿真器和真实外设硬件协同工作，以实现固件的动态分析。AVATAR 的核心理念是在仿真器中执行固件的同时，将I&#x2F;O访问请求转发给真实的嵌入式设备，从而提高仿真精度和性能。</li></ol><p>技术原理：<br>AVATAR 的工作机制包括：</p><p>I&#x2F;O 转发：当仿真器中运行的固件发出I&#x2F;O访问指令时，AVATAR 会将这些访问请求转发给真实的外设设备。例如，访问网络接口、存储设备或传感器。<br>动态优化：AVATAR 动态优化代码和数据在仿真环境和真实硬件之间的分布。仿真器处理固件的主要逻辑部分，而外设交互则委托给真实的嵌入式设备。这种分布优化使得仿真性能显著提高。<br>优势：<br>AVATAR 不仅能够提高仿真的效率，还可以在仿真中实现动态分析。它能够捕获固件的执行路径、检测潜在漏洞，并通过外设的真实行为进行测试。由于部分功能是在真实硬件上执行的，AVATAR 能够提供与完全仿真器无法实现的高精度分析。</p><ol start="3"><li>SURROGATES<br>概述：<br>SURROGATES 是基于 AVATAR 的进一步优化版本，它通过加强外部设备与固件的连接来提升仿真的效率和稳定性。SURROGATES 专注于解决仿真中常见的中断处理、DMA（直接内存访问）以及时钟同步等问题。</li></ol><p>技术原理：<br>SURROGATES 的关键创新点是使用了自定义的低延迟 FPGA 桥接器。它将宿主机的外围组件互连（PCI）总线与待测试系统连接起来，使得仿真器能够更加有效地访问固件的外设。</p><p>低延迟通信：FPGA 桥接器大大减少了仿真环境与真实外设之间的通信延迟，从而增强了仿真的实时性和精确性。<br>问题解决：SURROGATES 通过优化整个仿真系统，解决了以前仿真器中的常见问题，例如中断处理不准确、DMA 操作缺失以及时钟同步不良等。这使得仿真器在更复杂的环境中也能稳定运行。<br>应用场景：<br>SURROGATES 在需要极高仿真精度的场景中表现尤为突出。例如，它能够有效仿真高速数据传输设备，复杂的多设备通信系统等。通过优化外设的仿真和硬件访问，SURROGATES 能够支持更多类型的嵌入式设备仿真，并提高分析效率。</p><ol start="4"><li>ECMO<br>概述：<br>ECMO 是另一种基于部分仿真的系统，它提出了一种外设移植技术，目的是减少在QEMU等仿真器中手动添加外设仿真模型的复杂性。</li></ol><p>技术原理：<br>ECMO 的核心思想是将特定外设的驱动程序移植到目标固件的内核二进制文件中。它不需要为每个外设编写专门的仿真代码，而是通过将真实外设的驱动直接嵌入固件，使得仿真器能够更好地处理外设交互。</p><p>外设移植：ECMO 将外设的仿真模型和驱动程序移植到固件内核中，从而简化了复杂外设的仿真过程。这种方法使仿真器能够无缝访问外设，而不需要在仿真器中逐一实现每个外设的行为。<br>双组件移植：ECMO 通过将外设的仿真模型移植到 QEMU，同时将设备驱动程序移植到固件内核中，成功减少了手动仿真代码编写的需求。<br>优势：<br>ECMO 的移植技术有效解决了仿真中外设复杂性的瓶颈。它减少了在仿真器中逐个实现外设功能的需求，能够加快仿真开发的速度，并且能够更灵活地处理复杂的固件与外设交互。</p><p>总结<br>以上几种部分仿真方法通过不同的技术手段，解决了嵌入式设备外设仿真难的问题。PROSPECT 提供了透明的外设访问转发机制，AVATAR 通过动态优化提高了仿真效率，SURROGATES 通过低延迟硬件桥接器和系统优化增强了稳定性，而 ECMO 则通过外设驱动移植简化了仿真配置。这些方法的结合为嵌入式设备的固件分析和漏洞挖掘提供了强大的工具支持，尤其在无法完全仿真外设时，部分仿真成为了非常有效的替代方案。</p><h3 id="完全仿真"><a href="#完全仿真" class="headerlink" title="完全仿真"></a>完全仿真</h3><p>为了完全模拟固件，Firmadyne [22] 从设备中提取文件系统，并将其放入与 QEMU 一起运行的预编译通用 Linux 内核中。由于全仿真的可扩展性，Firmadyne 可以对目标固件进行大规模测试。 Firmadyne 在 69 个固件映像上发现了 14 个未知漏洞。 ARM-X[95]在仿真方面采用了与Fimadyne类似的思路，但需要用户提供更多的信息和配置。而且ARM-X只能用于ARM架构的设备，并且需要固件中的rootfs和NVRAM作为支持。 Pretender [63] 是一个在虚拟环境中自动重新托管各种嵌入式系统固件的框架。它记录物理硬件和固件之间的交互，然后使用这些记录通过机器学习来构建模型来描述每个外围设备。因此，Pretender可以完全将固件放置在虚拟化环境中，不需要保持对硬件设备的长期访问。</p><h2 id="大规模测试"><a href="#大规模测试" class="headerlink" title="大规模测试"></a>大规模测试</h2><p>在模型外围设备时，有些物联网设备系统没有外围设备，但有些系统可能连接到可编程逻辑控制器（PLC）、FPGA、传感器、数据库和许多其他外围设备。许多部分仿真方法通常需要手动操作，其中包括将方法扩展到大规模测试的繁重工程工作，因此会存在挑战性。</p><p>目前有多种方法来帮助进行大规模测试，Laelaps 引入符号执行协助外设仿真，可以在无需先验知识的情况下运行各种固件，但只能在短期执行（并不贯穿固件的整个运行，而是只在关键点执行），路径过长会存在依赖爆炸的问题。P2IM是一个无需硬件即可独立测试固件的软件框架。它抽象外围设备并根据自动生成的模型，动态处理固件 I&#x2F;O。它将目标固件及其内存映射作为输入，并通过将现成的模糊器（即 AFL）的输入提供给外围设备来模糊代码。</p><h2 id="检测漏洞"><a href="#检测漏洞" class="headerlink" title="检测漏洞"></a>检测漏洞</h2><h3 id="漏洞检测的挑战"><a href="#漏洞检测的挑战" class="headerlink" title="漏洞检测的挑战"></a>漏洞检测的挑战</h3><p>在虚拟环境中进行漏洞检测时，研究人员面临多个挑战。虽然仿真技术可以帮助更好地执行固件的安全分析，但由于各种限制，这些漏洞检测的应用仍然存在一定的困难。这些挑战包括：</p><ul><li>驱动程序中的错误处理代码难以触发，通常在极端情况下才会暴露漏洞。</li><li>某些物联网设备中嵌入的Web应用服务可能存在大量隐藏的安全漏洞。</li><li>针对设备与硬件交互边界的漏洞检测通常不涉及系统调用，难以在现有检测方法中被发现。</li></ul><h3 id="漏洞检测的技术与方法"><a href="#漏洞检测的技术与方法" class="headerlink" title="漏洞检测的技术与方法"></a>漏洞检测的技术与方法</h3><p>为了克服这些挑战，研究人员提出了以下几种主要技术和方法：</p><h4 id="（1）驱动程序中的错误处理检测（Error-Handling-in-Driver）"><a href="#（1）驱动程序中的错误处理检测（Error-Handling-in-Driver）" class="headerlink" title="（1）驱动程序中的错误处理检测（Error Handling in Driver）"></a>（1）<strong>驱动程序中的错误处理检测（Error Handling in Driver）</strong></h4><ul><li><strong>SymDrive</strong>：这一框架通过符号执行来测试Linux设备驱动程序。它使用静态分析与源代码转换，显著减少了测试新驱动程序时所需的手动调整工作。SymDrive会自动化检测固件中设备驱动程序的错误处理部分。</li><li><strong>IFIZZ</strong>：通过模糊测试来专门针对错误处理代码进行检测。它采用状态感知和受限错误生成的方式，有效地覆盖了更深的错误路径。</li><li><strong>FIFUZZ</strong>：这是另一个模糊测试框架，它通过上下文敏感的故障注入方法，能够在复杂触发场景下找到隐藏的深层次错误。</li></ul><h4 id="（2）Web接口漏洞检测（Web-Interface-Vulnerability-Detection）"><a href="#（2）Web接口漏洞检测（Web-Interface-Vulnerability-Detection）" class="headerlink" title="（2）Web接口漏洞检测（Web Interface Vulnerability Detection）"></a>（2）<strong>Web接口漏洞检测（Web Interface Vulnerability Detection）</strong></h4><ul><li><strong>FIRMADYNE</strong>：该框架通过自动化动态分析方法检测嵌入式固件中的内置Web应用服务漏洞。它通过仿真运行固件，并自动检测固件中Web接口的安全漏洞，如缓冲区溢出、命令注入等问题。</li><li><strong>FirmFuzz</strong>：一个专门为Linux固件设计的独立仿真与自动化动态分析框架。它使用基于灰盒的生成式模糊测试方法，结合静态分析和系统监控来检测固件中Web应用程序的漏洞。</li></ul><h4 id="（3）其他检测技术（Other-Detection-Techniques）"><a href="#（3）其他检测技术（Other-Detection-Techniques）" class="headerlink" title="（3）其他检测技术（Other Detection Techniques）"></a>（3）<strong>其他检测技术（Other Detection Techniques）</strong></h4><ul><li><strong>FIRM-AFL</strong>：通过增强的进程仿真技术来解决系统仿真中的性能瓶颈，并启用POSIX兼容的固件模糊测试。相比全系统仿真，FIRM-AFL的检测吞吐量平均提升了8.2倍。</li><li><strong>PeriScope</strong>：这一方法用于检测设备与硬件交互边界的漏洞。PeriScope通过挂钩固件内核的页面错误处理机制，检测和记录设备驱动与硬件之间的通信流量。它还引入了PeriFuzz框架，用于模拟针对外围设备的攻击。</li></ul><p>现在主要讨论驱动程序中的错误处理检测方法，详细分析 FIFUZZ 的具体实现。</p><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/c9ca1e83.png"></p><h3 id=""><a href="#" class="headerlink" title=""></a></h3>]]></content>
    
    
    <categories>
      
      <category>论文笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>漏洞挖掘</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于生成式人工智能增强手写数字识别的实验研究</title>
    <link href="/2025/01/16/%E9%AB%98%E7%BA%A7%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/"/>
    <url>/2025/01/16/%E9%AB%98%E7%BA%A7%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/</url>
    
    <content type="html"><![CDATA[<h1 id="基于生成式人工智能增强手写数字识别的实验研究"><a href="#基于生成式人工智能增强手写数字识别的实验研究" class="headerlink" title="基于生成式人工智能增强手写数字识别的实验研究"></a>基于生成式人工智能增强手写数字识别的实验研究</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>本研究探索了三种不同的生成式神经网络方法（DDPM、VAE 和 GAN）在增强手写数字识别任务中的应用。通过对比实验表明，生成式模型能够有效提升分类器的性能。实验结果显示，DDPM方法在样本质量和分类增强效果上表现最优，而 VAE 方法在训练效率上具有优势。</p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><h3 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h3><p>手写数字识别是计算机视觉领域的基础任务，但传统方法在数据量有限的情况下往往表现不佳。生成式模型的出现为解决此类问题提供了新的思路。</p><h3 id="研究目的"><a href="#研究目的" class="headerlink" title="研究目的"></a>研究目的</h3><ul><li>探索不同生成式模型在数据增强中的效果</li><li>比较各种方法的优劣势</li><li>提出最优的模型组合方案</li></ul><h2 id="系统设计"><a href="#系统设计" class="headerlink" title="系统设计"></a>系统设计</h2><p>系统采用模块化设计，主要包含四个核心组件：</p><ol><li><strong>生成模型模块</strong></li><li><strong>分类器模块</strong></li><li><strong>评估与分析模块</strong></li></ol><h3 id="DDPM模型"><a href="#DDPM模型" class="headerlink" title="DDPM模型"></a>DDPM模型</h3><p>DDPM（Denoising Diffusion Probabilistic Models）模型采用了基于马尔可夫链的扩散过程，主要包含两个核心过程：</p><ol><li><strong>前向扩散过程</strong>：逐步向图像添加高斯噪声</li><li><strong>反向扩散过程</strong>：学习去噪过程，逐步从噪声恢复图像</li></ol><h4 id="前向过程"><a href="#前向过程" class="headerlink" title="前向过程"></a>前向过程</h4><p>前向过程的公式如下所示：</p><p>$<br>x_t &#x3D; \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon<br> $</p><ul><li>$ x_t $：在时间步 $ t $ 的数据。</li><li>$ \bar{\alpha}<em>t &#x3D; \prod</em>{i&#x3D;1}^t \alpha_i $：累积噪声控制参数。</li><li>$ \epsilon $：从标准正态分布中采样的噪声。</li></ul><h4 id="反向过程"><a href="#反向过程" class="headerlink" title="反向过程"></a>反向过程</h4><p>$<br>x_{t-1} &#x3D; \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta \right) + \sqrt{\beta_t} z<br> $</p><ul><li>$ x_{t-1} $：还原到前一步的数据。</li><li>$ \epsilon_\theta $：由神经网络预测的噪声。</li><li>$ z $：附加的随机噪声，仅在 $ t &gt; 0 $ 时加入。</li></ul><h4 id="核心代码"><a href="#核心代码" class="headerlink" title="核心代码"></a>核心代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DDPM</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, device, n_steps=<span class="hljs-number">1000</span>, min_beta=<span class="hljs-number">0.0001</span>, max_beta=<span class="hljs-number">0.02</span></span>):<br>        <span class="hljs-comment"># 初始化扩散过程参数</span><br>        <span class="hljs-variable language_">self</span>.n_steps = n_steps<br>        <span class="hljs-variable language_">self</span>.device = device<br>        <span class="hljs-variable language_">self</span>.beta = torch.linspace(min_beta, max_beta, n_steps).to(device)<br>        <span class="hljs-variable language_">self</span>.alpha = <span class="hljs-number">1</span> - <span class="hljs-variable language_">self</span>.beta<br>        <span class="hljs-variable language_">self</span>.alpha_bar = torch.cumprod(<span class="hljs-variable language_">self</span>.alpha, dim=<span class="hljs-number">0</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sample_forward</span>(<span class="hljs-params">self, x0, t</span>):<br>        <span class="hljs-comment"># 前向过程：添加噪声</span><br>        alpha_bar = <span class="hljs-variable language_">self</span>.alpha_bar[t].reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        eps = torch.randn_like(x0)<br>        <span class="hljs-keyword">return</span> torch.sqrt(alpha_bar) * x0 + torch.sqrt(<span class="hljs-number">1</span> - alpha_bar) * eps, eps<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sample_backward</span>(<span class="hljs-params">self, shape, net, device, return_sequence=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-comment"># 反向过程：从噪声生成数据</span><br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            x = torch.randn(shape).to(device)<br>            sequence = []<br>            <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">reversed</span>(<span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.n_steps)), desc=<span class="hljs-string">&quot;Sampling&quot;</span>):<br>                <span class="hljs-keyword">if</span> t % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>                    torch.cuda.empty_cache()<br>                t_batch = torch.ones(shape[<span class="hljs-number">0</span>], dtype=torch.long, device=device) * t<br>                eps_theta = net(x, t_batch)<br>                alpha = <span class="hljs-variable language_">self</span>.alpha[t].to(device)<br>                alpha_bar = <span class="hljs-variable language_">self</span>.alpha_bar[t].to(device)<br>                beta = <span class="hljs-variable language_">self</span>.beta[t].to(device)<br>                factor1 = (<span class="hljs-number">1</span> / torch.sqrt(alpha))<br>                factor2 = (beta / torch.sqrt(<span class="hljs-number">1</span> - alpha_bar))<br>                z = torch.randn_like(x) <span class="hljs-keyword">if</span> t &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span><br>                x = factor1 * (x - factor2 * eps_theta) + torch.sqrt(beta) * z<br>                <span class="hljs-keyword">if</span> return_sequence <span class="hljs-keyword">and</span> t % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>                    sequence.append(x.detach().cpu())<br>                <span class="hljs-keyword">del</span> eps_theta<br>                torch.cuda.empty_cache()<br>            <span class="hljs-keyword">return</span> sequence <span class="hljs-keyword">if</span> return_sequence <span class="hljs-keyword">else</span> x<br></code></pre></td></tr></table></figure><h3 id="VAE-模型"><a href="#VAE-模型" class="headerlink" title="VAE 模型"></a>VAE 模型</h3><p>VAE（Variational AutoEncoder，变分自编码器）是一种生成模型，通过编码器将输入数据映射到潜空间，并通过重参数化技巧采样潜变量，再使用解码器将潜变量重构为原始数据。</p><p>VAE 的主要过程包括：</p><ol><li><strong>编码过程</strong>：输入数据通过卷积层降维，提取特征，并生成潜空间的均值和对数方差。</li><li><strong>重参数化技巧</strong>：对潜空间进行采样，以确保模型的可导性。</li><li><strong>解码过程</strong>：从潜空间的样本生成数据，通过反卷积逐步还原为输入尺寸。</li></ol><hr><h4 id="编码过程"><a href="#编码过程" class="headerlink" title="编码过程"></a>编码过程</h4><p>编码过程通过卷积层逐步提取输入数据的特征，并最终生成潜空间的均值 $ \mu $ 和对数方差 $ \log \sigma^2 $。</p><ul><li><strong>均值</strong>：</li></ul><p>$ \mu &#x3D; \text{fc}_\mu(\text{flatten}(\text{encoder}(x))) $</p><ul><li><strong>对数方差</strong>：</li></ul><p>$ \log \sigma^2 &#x3D; \text{fc}_\text{var}(\text{flatten}(\text{encoder}(x))) $</p><ul><li>$ x $：输入数据。</li><li>$ \mu $：潜空间的均值。</li><li>$ \log \sigma^2 $：潜空间的对数方差。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">self, x</span>):<br>    x = <span class="hljs-variable language_">self</span>.encoder(x)  <span class="hljs-comment"># 提取特征</span><br>    x = torch.flatten(x, start_dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># 平铺为向量</span><br>    mu = <span class="hljs-variable language_">self</span>.fc_mu(x)  <span class="hljs-comment"># 计算均值</span><br>    log_var = <span class="hljs-variable language_">self</span>.fc_var(x)  <span class="hljs-comment"># 计算对数方差</span><br>    <span class="hljs-keyword">return</span> mu, log_var<br></code></pre></td></tr></table></figure><h4 id="重参数化技巧"><a href="#重参数化技巧" class="headerlink" title="重参数化技巧"></a>重参数化技巧</h4><p>重参数化技巧用于从潜空间中采样，以确保模型的可导性。公式如下：</p><p>$ z &#x3D; \mu + \epsilon \cdot \sigma, \quad \epsilon \sim \mathcal{N}(0, I), \quad \sigma &#x3D; \exp(0.5 \cdot \log \sigma^2) $</p><ul><li>$ z $：从潜空间采样的潜变量。</li><li>$ \mu $：潜空间的均值，由编码器输出。</li><li>$ \sigma $：潜空间的标准差，通过对数方差 $ \log \sigma^2 $ 计算。</li><li>$ \epsilon $：标准正态分布噪声，用于引入随机性。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">reparameterize</span>(<span class="hljs-params">self, mu, log_var</span>):<br>    std = torch.exp(<span class="hljs-number">0.5</span> * log_var)  <span class="hljs-comment"># 计算标准差</span><br>    eps = torch.randn_like(std)    <span class="hljs-comment"># 从标准正态分布采样噪声</span><br>    <span class="hljs-keyword">return</span> mu + eps * std          <span class="hljs-comment"># 重参数化</span><br></code></pre></td></tr></table></figure><h4 id="解码过程"><a href="#解码过程" class="headerlink" title="解码过程"></a>解码过程</h4><p>解码过程将潜空间的潜变量 $ z $ 转换回原始数据尺寸，主要包括以下步骤：</p><ol><li>特征向量映射</li></ol><p>将潜变量 $ z $ 映射到特征空间的向量形式，便于后续的卷积操作：</p><p>$ h &#x3D; \text{decoder_input}(z) $</p><ul><li>$ z $：从潜空间采样的潜变量。</li><li>$ h $：解码器输入的特征向量。</li></ul><ol start="2"><li>特征向量重塑</li></ol><p>将映射后的特征向量调整为适配卷积输入的形状：</p><p>$ h &#x3D; \text{reshape}(h, [\text{batch_size}, \text{channels}, \text{height}, \text{width}]) $</p><ol start="3"><li>卷积操作</li></ol><p>通过反卷积逐步放大特征图，最终恢复到输入数据的尺寸：</p><p>$ \hat{x} &#x3D; \text{decoder}(h) $</p><ol start="4"><li>自适应池化与映射</li></ol><p>在最终输出阶段，利用自适应池化和卷积层精确调整数据尺寸为目标尺寸：</p><p>$ \hat{x} &#x3D; \text{AdaptiveAvgPool2d}(h) \rightarrow \text{Conv2d}(\text{to desired output channels}) $</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">decode</span>(<span class="hljs-params">self, z</span>):<br>    <span class="hljs-comment"># 将潜变量映射为特征向量</span><br>    x = <span class="hljs-variable language_">self</span>.decoder_input(z)<br>    <br>    <span class="hljs-comment"># 重塑为卷积输入形状</span><br>    x = x.view(-<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.hidden_dims[-<span class="hljs-number">1</span>], <span class="hljs-variable language_">self</span>.feature_size, <span class="hljs-variable language_">self</span>.feature_size)<br>    <br>    <span class="hljs-comment"># 通过解码器逐步还原</span><br>    x = <span class="hljs-variable language_">self</span>.decoder(x)<br>    <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><h2 id="3-实验设置"><a href="#3-实验设置" class="headerlink" title="3. 实验设置"></a>3. 实验设置</h2><h3 id="3-1-数据集"><a href="#3-1-数据集" class="headerlink" title="3.1 数据集"></a>3.1 数据集</h3><ul><li><strong>训练集</strong>：<ul><li>MNIST训练集（60,000张）</li><li>生成样本（每种方法20,000张）</li></ul></li><li><strong>测试集</strong>：<ul><li>MNIST测试集（10,000张）</li></ul></li></ul><h3 id="3-2-实验环境"><a href="#3-2-实验环境" class="headerlink" title="3.2 实验环境"></a>3.2 实验环境</h3><ul><li>GPU：NVIDIA RTX 4090</li><li>框架：PyTorch 2.4.0</li><li>CUDA：12.4</li></ul><h2 id="4-实验结果与分析"><a href="#4-实验结果与分析" class="headerlink" title="4. 实验结果与分析"></a>4. 实验结果与分析</h2><p>本实验还实现了 GAN 版本的数据增强方法，实验对比分析如下。</p><h3 id="4-2-分类性能提升"><a href="#4-2-分类性能提升" class="headerlink" title="4.2 分类性能提升"></a>4.2 分类性能提升</h3><table><thead><tr><th>增强方法</th><th>基准准确率</th><th>增强后准确率</th><th>提升幅度</th></tr></thead><tbody><tr><td>无增强</td><td>97.25%</td><td>-</td><td>-</td></tr><tr><td>DDPM增强</td><td>97.25%</td><td>98.45%</td><td>1.20%</td></tr><tr><td>GAN增强</td><td>97.25%</td><td>98.12%</td><td>0.87%</td></tr><tr><td>VAE增强</td><td>97.25%</td><td>97.89%</td><td>0.64%</td></tr></tbody></table><h3 id="4-3-结果分析"><a href="#4-3-结果分析" class="headerlink" title="4.3 结果分析"></a>4.3 结果分析</h3><ol><li><strong>生成质量</strong>：<ul><li>DDPM生成的样本质量最高，但计算开销大</li><li>VAE生成样本相对模糊</li></ul></li><li><strong>分类增强效果</strong>：<ul><li>所有方法都能提升分类性能</li><li>DDPM增强效果最显著</li><li>增强效果与生成样本质量正相关</li></ul></li><li><strong>实际应用考虑</strong>：<ul><li>资源充足时推荐DDPM</li><li>稳定性和实时性要求高时推荐VAE</li></ul></li></ol><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h3 id="代码使用说明"><a href="#代码使用说明" class="headerlink" title="代码使用说明"></a>代码使用说明</h3><h4 id="目录结构"><a href="#目录结构" class="headerlink" title="目录结构"></a>目录结构</h4><p>├── config&#x2F;</p><p>│ ├── ddpm_config.py # DDPM配置</p><p>│ ├── gan_config.py # GAN配置</p><p>│ └── vae_config.py # VAE配置</p><p>├── models&#x2F;</p><p>│ ├── ddpm.py # DDPM模型实现</p><p>│ ├── gan.py # GAN模型实现</p><p>│ └── vae.py # VAE模型实现</p><p>├── utils&#x2F;</p><p>│ ├── data_loader.py # 数据加载工具</p><p>│ └── visualization.py # 可视化工具</p><p>├── train_ddpm.py # DDPM训练脚本</p><p>├── train_gan.py # GAN训练脚本</p><p>└── train_vae.py # VAE训练脚本</p><p>└── main.py # 启动脚本</p><h4 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h4><p>通过以下指令配置虚拟环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install requirements.txt<br></code></pre></td></tr></table></figure><h4 id="使用说明"><a href="#使用说明" class="headerlink" title="使用说明"></a>使用说明</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    <span class="hljs-comment"># 解析参数</span><br>    args = parse_args()<br>    <br>    <span class="hljs-comment"># 设置设备</span><br>    torch.cuda.set_device(args.device)<br>    <br>    <span class="hljs-comment"># 根据method参数选择运行的实验</span><br>    results = &#123;&#125;<br>    <br>    <span class="hljs-keyword">if</span> args.method <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;ddpm&#x27;</span>, <span class="hljs-string">&#x27;all&#x27;</span>]:<br>        results[<span class="hljs-string">&#x27;ddpm&#x27;</span>] = run_ddpm_experiment(args.train)<br>        <br>    <span class="hljs-keyword">if</span> args.method <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;vae&#x27;</span>, <span class="hljs-string">&#x27;all&#x27;</span>]:<br>        results[<span class="hljs-string">&#x27;vae&#x27;</span>] = run_vae_experiment(args.train)<br>        <br>    <span class="hljs-keyword">if</span> args.method <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;gan&#x27;</span>, <span class="hljs-string">&#x27;all&#x27;</span>]:<br>        results[<span class="hljs-string">&#x27;gan&#x27;</span>] = run_gan_experiment(args.train)<br></code></pre></td></tr></table></figure><p>使用以下指令来运行实验 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python ./main.py --method gan<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/7edf253a.png"></p>]]></content>
    
    
    <categories>
      
      <category>实验报告</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PRES阅读笔记</title>
    <link href="/2025/01/16/PRES%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <url>/2025/01/16/PRES%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>基于内存的动态图神经网络 (MDGNN)十分有效，但是存在一个比较致命的问题：</p><p><strong>在一个时间窗口内的事件，其中的时间依赖关系无法捕捉</strong></p><h2 id="背景知识（TGN的memory机制解释）"><a href="#背景知识（TGN的memory机制解释）" class="headerlink" title="背景知识（TGN的memory机制解释）"></a>背景知识（TGN的memory机制解释）</h2><p>之前实现的MAGIC，是在静态图上进行处理，一次性加载完所有的数据。而动态图的特点在于动态，图的信息是不断扩展动态变化的，图并不是一次全部加载完全，而是每次只加载一个时间窗口中的事件，也就是边。</p><p>那么如何去捕获每个窗口之间的信息呢，在TGN中，它通过在每个时间窗口中计算节点的状态（这个状态由当前窗口中的事件以及上一时刻的状态决定），这个状态就称作memory，这个memory就代表了这个节点的历史信息，这样就能够捕获每个节点从出生到当前状态的所有信息。</p><p>下面详细解释一下节点memory的更新过程：</p><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/92c8f93e.png" alt="图1"></p><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/2d88de91.png" alt="图2"></p><p>如图2所示，整个TGN的核心就在这个TGNMemory，它维护了三个结构，分别是memory、msg_s_store（源节点的msg存储）和msg_d_store（目标节点的msg存储）。先计算msg：对于batch中的src和dst分别计算一次msg（视为无向图），每次msg计算如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">raw_msg = torch.cat(raw_msg, dim=<span class="hljs-number">0</span>)<br>t_rel = t - <span class="hljs-variable language_">self</span>.last_update[src]<br>t_enc = <span class="hljs-variable language_">self</span>.time_enc(t_rel.to(raw_msg.dtype))<br><br>msg = msg_module(<span class="hljs-variable language_">self</span>.memory[src], <span class="hljs-variable language_">self</span>.memory[dst], raw_msg, t_enc)<br></code></pre></td></tr></table></figure><p>这里的raw_msg就是msg_s_store或msg_d_store，取决于是计算src的msg还是dst的msg</p><p>计算完msg之后，在当前时间窗口中，对每个节点聚合和它交互的边，这个聚合操作可以自定义，在TGN中它的实现是选择最新的那条边，其他边就扔掉了（这也就是PERS这篇论文中想解决的问题，处理一个时间窗口中的时间顺序关系），将选择的那条边对应的msg和当前状态的memory一起送进GRU（一种循环神经网络，可以捕获msg和memory的序列特征）中，来更新memory。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="现有工作存在的问题"><a href="#现有工作存在的问题" class="headerlink" title="现有工作存在的问题"></a>现有工作存在的问题</h3><ol><li>MDGNN无法维持同一批次（时间窗口）内的数据点之间的时间依赖性，因此常常选择较小的时间窗口。</li><li>较小的时间窗口无法有效利用计算资源，因此解决批量大小瓶颈对于计算资源的利用至关重要。</li></ol><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/f616b3f1.png"></p><h3 id="本文完成的工作"><a href="#本文完成的工作" class="headerlink" title="本文完成的工作"></a>本文完成的工作</h3><ol><li>进行了数学分析，证明了大时间窗口比小时间窗口更好（全文基本上都是数学分析，公式占了一半的篇幅，实在是看不太明白）</li><li>设计了一种新颖的框架来改善小时间窗口的问题</li><li>进行了实验，证明了本文提出的工作效果好</li></ol><h2 id="PERS-METHOD"><a href="#PERS-METHOD" class="headerlink" title="PERS METHOD"></a>PERS METHOD</h2><h3 id="方法总览"><a href="#方法总览" class="headerlink" title="方法总览"></a>方法总览</h3><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/02ce2897.png"></p><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/4076c791.png"></p><p>整篇文章的目的是对上式的右半边进行优化，右半边的值越小越好，其中最具影响力的两个因素是μ和σ<sub>max</sub>，因此本文就是为了去增大μ并减小σ<sub>max</sub>，提出了两个方向的改进措施<sup></sup>。</p><ul><li>ITERATIVE PREDICTION-CORRECTION SCHEME（优化第二项）</li><li>MEMORY COHERENCE SMOOTHING（优化第一项）</li></ul><p>伪代码如下：</p><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/bb9aa082.png"></p><h3 id="ITERATIVE-PREDICTION-CORRECTION-SCHEME（迭代预测校正）"><a href="#ITERATIVE-PREDICTION-CORRECTION-SCHEME（迭代预测校正）" class="headerlink" title="ITERATIVE PREDICTION-CORRECTION SCHEME（迭代预测校正）"></a>ITERATIVE PREDICTION-CORRECTION SCHEME（迭代预测校正）</h3><h4 id="概念定义：pending-events"><a href="#概念定义：pending-events" class="headerlink" title="概念定义：pending events"></a>概念定义：pending events</h4><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/8ad26fbb.png"></p><h4 id="背景知识：Gaussian-Mixture-Model"><a href="#背景知识：Gaussian-Mixture-Model" class="headerlink" title="背景知识：Gaussian Mixture Model"></a>背景知识：Gaussian Mixture Model</h4><p> GMM 假设数据点由多个高斯分布混合生成。每个高斯分布称为一个成分（component）。GMM 的概率密度函数是这些成分的加权和：  </p><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/20f8e7aa.png"></p><p>因此GMM的参数包括：</p><ul><li>每个成分的均值向量 μ<sub>i</sub> </li><li>每个成分的协方差矩阵 Σ<sub>i</sub> </li><li>每个成分的权重 π<sub>i</sub></li></ul><p>接着使用 期望最大化（EM）算法来更新参数，其主要包含两个主要步骤：</p><ul><li><strong>E 步（期望步）</strong>：计算每个数据点属于每个成分的概率，即责任度（responsibility）。</li><li><strong>M 步（最大化步）</strong>：根据 E 步计算的责任度，重新估计参数。</li></ul><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/e8cddf5d.png"></p><h4 id="纠错机制"><a href="#纠错机制" class="headerlink" title="纠错机制"></a>纠错机制</h4><p>设置GMM的成分数为2，由于该论文中的场景是进行链接预测，因此它会进行正负采样，所以这里设置成分数为2，不太懂这个是怎么训练的，但是最终，这个GMM的作用是预测δ<sub>si</sub> 的分布，通过这个分布可以每次去生成一个预测的节点状态：</p><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/774320c1.png"></p><p>之后为了拟合真实的考虑了时间的节点状态，作者使用参数γ去中和：</p><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/e30f87e2.png"></p><p>这时得到的值就是进行纠错之后的节点状态，那么为什么是上面两个公式这样的形式呢，作者在附录中进行了推导（作者说明了这里的符号和论文其他部分不一致），证明了：</p><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/6eeb6afa.png"></p><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/3f208881.png"></p><p>也就是表明本文方法计算出的状态相对于未改进之前，更接近真实值，优化了方差的大小（不懂那个方差是啥，但它说是就是吧）</p><h3 id="MEMORY-COHERENCE-SMOOTHING"><a href="#MEMORY-COHERENCE-SMOOTHING" class="headerlink" title="MEMORY COHERENCE SMOOTHING"></a>MEMORY COHERENCE SMOOTHING</h3><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/14c6d51b.png"></p><p>这个内存一致性怎么计算的看不懂，也不太能看懂是个啥，看它的描述，上半部分应该是考虑了pending events时（考虑了时间依赖性）去更新节点状态计算的梯度和未考虑pending events去更新节点状态计算的梯度进行点积。也就是说，这个一致性在直观上来讲，它度量了理想的梯度下降方向和实际的梯度下降方向的一致程度。当它是正值的时候，表示方向一致（可能参数都是增加或者较小？），负值就表示方向相反。</p>]]></content>
    
    
    <categories>
      
      <category>论文笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AMCNet阅读笔记</title>
    <link href="/2025/01/16/AMCNet%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <url>/2025/01/16/AMCNet%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>动态链接预测（在动态图中，预测两个点在时间t是否存在边）关注两个信息：结构信息和时间信息。现有工作要么只考虑其中一个因素，要么两个因素都单独考虑而没有将它们联系起来，忽略了两者之间的关联性。</p><p>因此本文提出了一种方法，<strong>多尺度注意力协同进化网络（Attentional Multi-scale Co-evolving Network）</strong>，使用多层次注意力机制的序列到序列的模型（<strong>sequence-to-sequence</strong>）来学习不同层次之间的动态变化的内在关系。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/9a2f5847.png"></p><p>本文中将图中的结构分为三个层次：</p><ul><li>微观层次（Microscopic structure）：图中的节点和边</li><li>中观层次（Mesoscopic structure）：图中的子图或者社区</li><li>宏观层次（Macroscopic structure）：整张图</li></ul><p>本文认为网络是由个体及其相互联系构成的，因此宏观时间动态自然取决于每个个体如何选择与他人建立联系的微观时间动态。另一方面，研究表明，人类行为受到社会关系的动态影响，例如政治取向、音乐品味，甚至人们如何选择新朋友。（中观影响微观，微观影响中观和宏观）</p><h3 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h3><ul><li>设计了多层次表示学习模块（通过GNN学习到节点表示，再通过节点表示来设计pooling表示中观和宏观），为了表示中观，提出了一个<code>motifs</code>的概念</li><li>提出了一个多层次协同进化模型来学习每个层次的动态特征</li><li>为了了解不同结构尺度的时间动态之间的内在相关性，通过一种新颖的基于注意力的分层模型，利用较高尺度的表示来指导较低尺度表示的学习过程。</li></ul><blockquote><p>motifs简介</p><p>motifs是一种模式，如下图所示：</p><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/d203283f.png"></p><p>它表示一种子图结构，如何去判定一个子图结构是否为motif呢？有两个指标：频率和重要性。</p><p>频率指的是它在图中出现的频率，重要性通过随机构造一张随机图，判断真实图种motif出现的频率和随机图出现的频率的差异来判断重要性，差异越大，越重要</p><p>这个motif的发现有现成的工作来做，本文中并没有涉及构造motif的代码，它直接用了</p></blockquote><h2 id="method"><a href="#method" class="headerlink" title="method"></a>method</h2><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/f7a04ef9.png"></p><p>整个架构分为两部分：</p><ol><li><strong>multiscale representation learning module</strong></li><li><strong>multi-scale evolving module.</strong></li></ol><h3 id="Multi-scale-Representation-Learning"><a href="#Multi-scale-Representation-Learning" class="headerlink" title="Multi-scale Representation Learning"></a>Multi-scale Representation Learning</h3><h4 id="Microscopic-Representation"><a href="#Microscopic-Representation" class="headerlink" title="Microscopic Representation"></a>Microscopic Representation</h4><p>用GAT来生成节点表示（就是MAGIC的骨架网络）</p><h4 id="Mesoscopic-Representation"><a href="#Mesoscopic-Representation" class="headerlink" title="Mesoscopic Representation"></a>Mesoscopic Representation</h4><p>作者一开始尝试，通过对motifs中的节点做平均池化来获得中观表示，但是效果不好，因此不单纯的做平均，加上了一个可学习的参数矩阵来为每个motif作为注意力参数</p><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/a089c176.png"></p><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/faf0efb3.png"></p><p>但是很奇怪，根据公式三，这样得到的表示不就是一个值了吗，我以为是每个motif都计算一个值作为中观表示</p><h4 id="Macroscopic-Representation"><a href="#Macroscopic-Representation" class="headerlink" title="Macroscopic Representation"></a>Macroscopic Representation</h4><p>全局的宏观表示就是将所有节点的特征平均一下：</p><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/7dd9d57b.png"></p><h3 id="Multi-scale-Co-evolving-module"><a href="#Multi-scale-Co-evolving-module" class="headerlink" title="Multi-scale Co-evolving module"></a>Multi-scale Co-evolving module</h3><p>本文提出了两个观点：</p><ol><li>首先，从信息论的角度来看，数据中的噪声量和信息聚合水平通常呈负相关。结构层次越高，信息越少，噪声也越小。因此，较高结构尺度的时间动态比较低结构尺度更可预测。因此，较高尺度的预测有助于纠正较低尺度预测的潜在系统偏差，这对学习模型施加了尺度不变的约束。</li><li>其次，不同结构尺度的信息捕获了图的不同特征，从而相互补充。对不同尺度的时间动态进行联合建模使模型能够利用来自不同上下文范围的信息来进行预测。</li></ol><h4 id="Impletation"><a href="#Impletation" class="headerlink" title="Impletation"></a>Impletation</h4><p><strong>Sequence to Sequence Backbone：</strong>使用序列模型来捕获每个层次的数据的内在结构，设计了三个：</p><ol><li><strong>seq-seq-node：</strong>Seq2Seq_Attention(enc, dec, dev)</li><li><strong>seq-seq-motif：</strong>Seq2Seq_Attention(enc, dec, dev)</li><li><strong>seq-seq-graph：</strong>Seq2Seq(enc, dec, dev)</li></ol><p>从graph层次开始，graph指导motif，motif指导node：</p><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/b681fa65.png"></p><p>其中<strong>seq2seq:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Seq2Seq</span>(torch.nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, encoder, decoder, device</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.encoder = encoder<br>        <span class="hljs-variable language_">self</span>.decoder = decoder<br>        <span class="hljs-variable language_">self</span>.device = device<br>        <span class="hljs-keyword">assert</span> encoder.hidden_size == decoder.hidden_size, <span class="hljs-string">&quot;Hidden dimensions of encoder and decoder must be equal!&quot;</span><br>        <span class="hljs-keyword">assert</span> encoder.n_layers == decoder.n_layers,       <span class="hljs-string">&quot;Encoder and decoder must have equal number of layers!&quot;</span><br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, y, teacher_forcing_ratio = <span class="hljs-number">0.75</span></span>):<br></code></pre></td></tr></table></figure><p>其中<strong>Seq2Seq_Attention:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Seq2Seq_Attention</span>(torch.nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, encoder, decoder, device</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.encoder = encoder<br>        <span class="hljs-variable language_">self</span>.decoder = decoder<br>        <span class="hljs-variable language_">self</span>.device = device<br>        <span class="hljs-variable language_">self</span>.attention = nn.Linear(encoder.hidden_size*<span class="hljs-number">2</span>, encoder.hidden_size)<br>        <span class="hljs-variable language_">self</span>.softmax = nn.Softmax(dim=<span class="hljs-number">1</span>)<br><br>        <span class="hljs-keyword">assert</span> encoder.hidden_size == decoder.hidden_size, <span class="hljs-string">&quot;Hidden dimensions of encoder and decoder must be equal!&quot;</span><br>        <span class="hljs-keyword">assert</span> encoder.n_layers == decoder.n_layers,       <span class="hljs-string">&quot;Encoder and decoder must have equal number of layers!&quot;</span><br>        nn.init.xavier_normal_(<span class="hljs-variable language_">self</span>.attention.weight)<br>        nn.init.constant_(<span class="hljs-variable language_">self</span>.attention.bias, <span class="hljs-number">0.0</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, y,hiddens, teacher_forcing_ratio = <span class="hljs-number">0.75</span></span>):<br></code></pre></td></tr></table></figure><p><strong>encoder（LSTM）：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Encoder</span>(torch.nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,</span><br><span class="hljs-params">                input_size = <span class="hljs-number">128</span>,</span><br><span class="hljs-params">                embedding_size = <span class="hljs-number">128</span>,</span><br><span class="hljs-params">                hidden_size = <span class="hljs-number">256</span>,</span><br><span class="hljs-params">                n_layers = <span class="hljs-number">2</span>,</span><br><span class="hljs-params">                dropout = <span class="hljs-number">0.5</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.hidden_size = hidden_size<br>        <span class="hljs-variable language_">self</span>.n_layers = n_layers<br>        <span class="hljs-variable language_">self</span>.linear = nn.Linear(input_size, embedding_size)<br>        <span class="hljs-variable language_">self</span>.rnn = nn.LSTM(embedding_size, hidden_size, n_layers,<br>                        dropout = dropout)<br>        <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(dropout)<br><br>        nn.init.xavier_normal_(<span class="hljs-variable language_">self</span>.linear.weight)<br>        nn.init.constant_(<span class="hljs-variable language_">self</span>.linear.bias, <span class="hljs-number">0.0</span>)<br></code></pre></td></tr></table></figure><p><strong>decoder（LSTM）：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Decoder</span>(torch.nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,</span><br><span class="hljs-params">                output_size = <span class="hljs-number">256</span>,</span><br><span class="hljs-params">                embedding_size = <span class="hljs-number">128</span>,</span><br><span class="hljs-params">                hidden_size = <span class="hljs-number">256</span>,</span><br><span class="hljs-params">                n_layers = <span class="hljs-number">4</span>,</span><br><span class="hljs-params">                dropout = <span class="hljs-number">0.5</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.output_size = output_size<br>        <span class="hljs-variable language_">self</span>.hidden_size = hidden_size<br>        <span class="hljs-variable language_">self</span>.n_layers = n_layers<br><br>        <span class="hljs-variable language_">self</span>.embedding = nn.Linear(output_size, embedding_size)<br>        <span class="hljs-variable language_">self</span>.rnn = nn.LSTM(embedding_size, hidden_size, n_layers, dropout = dropout)<br>        <span class="hljs-variable language_">self</span>.linear = nn.Linear(hidden_size, output_size)<br>        <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(dropout)<br><br>        nn.init.xavier_normal_(<span class="hljs-variable language_">self</span>.embedding.weight)<br>        nn.init.constant_(<span class="hljs-variable language_">self</span>.embedding.bias, <span class="hljs-number">0.0</span>)<br>        nn.init.xavier_normal_(<span class="hljs-variable language_">self</span>.linear.weight)<br>        nn.init.constant_(<span class="hljs-variable language_">self</span>.linear.bias, <span class="hljs-number">0.0</span>)<br></code></pre></td></tr></table></figure><p>由于是联合优化，因此总体Model如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model</span>(torch.nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, seq2seq_node, seq2seq_motif,seq2seq_graph, device</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.seq2seq_node = seq2seq_node<br>        <span class="hljs-variable language_">self</span>.seq2seq_motif = seq2seq_motif<br>        <span class="hljs-variable language_">self</span>.seq2seq_graph = seq2seq_graph<br>        <span class="hljs-variable language_">self</span>.device = device<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, node_x, motif_x,graph_x,node_y,motif_y,graph_y, teacher_forcing_ratio = <span class="hljs-number">0.75</span></span>):<br>        graph_emb,graph_hiddens=seq2seq_graph(graph_x,graph_y,teacher_forcing_ratio)<br>        motig_emb,motif_hiddens=seq2seq_motif(motif_x,motif_y,graph_hiddens,teacher_forcing_ratio)<br>        node_emb,_ =seq2seq_node(node_x,node_y,motif_hiddens,teacher_forcing_ratio)<br>        final_emb=torch.cat((node_emb,motig_emb,graph_emb),axis=<span class="hljs-number">2</span>)<br>        <span class="hljs-keyword">return</span> final_emb<br></code></pre></td></tr></table></figure><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>很有启发性的一篇工作，总体思路就是在AE的基础上，串联多层次AE，来融合不同层次的语义信息（通过在不同层次之间，将上层的输出来生成下层的注意力，进行语义融合）。</p><p>文章的writting和画图都很棒，很值得借鉴，就是开源的代码不完全，GAT部分没有给出，无法复现。</p>]]></content>
    
    
    <categories>
      
      <category>论文笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>动态图神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AFGRL阅读笔记</title>
    <link href="/2025/01/16/AFGRL%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <url>/2025/01/16/AFGRL%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>提出了一种新颖的无增强图自监督学习框架，节点粒度的工作（个人感觉已经不像是对比学习了，不需要负例，只需要最小化正例之间的距离）</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>在对比学习方法中，需要对数据进行增强来构造正负例，但是 image 和 graph 在数据特征上具有很大的不同：虽然增强在图像上得到了很好的定义，但它在图上的行为可能是任意的。例如，就图像而言，即使随机裁剪和旋转它们，或者扭曲它们的颜色，它们的底层语义也几乎没有改变。另一方面，当我们扰动（删除或添加）图的边&#x2F;节点及其特征时，我们无法确定增强图是否与原始图正相关，更糟糕的是，由于图很难可视化，因此我们也没办法直观的感受到增强的有效性。例如，在分子图中，从阿司匹林的苯环上去掉一个碳原子会破坏芳香系统并产生烯烃链。此外，扰乱阿司匹林的连接可能会引入一种性质完全不同的分子，即五元内酯，这些微小改动造成了巨大的变化。</p><p>然后作者还进行了实验对比，说明了先前的工作有效性与否，很大程度上取决于使用的数据增强方式</p><h2 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h2><p>不同于之前的对比学习方法，本文提出的方法<code>不构造</code>view，而是从图本身去找 view，那么如何去确保自己找的 view 是一个好的 view 呢，本文提出了考虑了两种找 view 的方式结合起来，这样找到的 view 就更可能是好 view。</p><h2 id="Impletation"><a href="#Impletation" class="headerlink" title="Impletation"></a>Impletation</h2><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/0584cdf7.png"></p><p>如上图所示，首先对一张图输入到两个 GCN 中，分别得到两个节点的嵌入H<sup>ξ</sup> 和 H<sup>θ</sup> ，这两者可以分别看做是初步找到的 view 和原始 view（分别记作 VE1 和 VE），接下来我们是要针对这个初步找到的 view 进行优化，让它更好。</p><h3 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h3><p>第一步我们对 VE 中的每个节点，去计算 VE1 中每个节点和该节点的距离：</p><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/b0ef9ecf.png"></p><p>然后找到 K 个最近的节点，作为第一步的结果（KNN，K nearest neighbor），假设对于节点 V<sub>i</sub> ，KNN 的结果为 B<sub>i</sub> </p><h3 id="Local-Structural-Information"><a href="#Local-Structural-Information" class="headerlink" title="Local Structural Information"></a>Local Structural Information</h3><p>首先作者做了实验拥挤，发现方法 1 的结果，随着 K 的增大，其中和目标节点共享同一标签的比率逐渐降低</p><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/20f4f5b0.png"></p><p>说明 1 的方法得到的结果存在一定的误差，为了缓解这一误差。作者考虑到了这一假设：</p><blockquote><p>对于节点 vi，其相邻节点 Ni 倾向于与查询节点 vi 共享相同的标签，即平滑假设（Zhu、Ghahramani 和 Lafferty 2003）。</p></blockquote><p>作者结合了 Local Structural Information，也就是说将目标节点的V<sub>i</sub> 的邻居节点集合 N<sub>i</sub> 和上一步得到的B<sub>i</sub> 作交集</p><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/a6f5721d.png"></p><h3 id="Global-Semantics"><a href="#Global-Semantics" class="headerlink" title="Global Semantics"></a>Global Semantics</h3><p>在之前的步骤中，我们的筛选找到了和目标节点相邻的、具有相似语义的节点，但是也存在这样一种情况，即和目标节点不相邻、具有语义相似的节点（在文中举例是，在引文网络中，两个学者的研究方向一致，但是两人的文章并没有引用关系）。</p><p>作者认为这种特征可以根据聚类来捕获。（这里是我的一个疑惑的地方，为什么聚类可以捕获这种特征）</p><p>具体做法是，针对之前得到的 H<sup>ξ</sup> 进行 Kmeans 聚类，得到很多个簇，假设我们的目标节点 <img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/d21e93a2.png">，那么和 v<sub>i </sub>属于同一个簇的所有节点我们记作 C<sub>i</sub> ，这里同样我们取交集</p><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/fdab93fb.png"></p><p>最终，我们通过上述两部分，得到了筛选过后的 VE1:</p><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/8b57fb69.png"></p><p>最终构造的正例：</p><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/7b3d5c13.png"></p><h2 id="Overall-Loss-function"><a href="#Overall-Loss-function" class="headerlink" title="Overall Loss function"></a>Overall Loss function</h2><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/43e0f731.png"></p><p>我们的目标函数旨在最小化查询节点 v<sub>i</sub> 与其正例 P<sub>i</sub> 之间的余弦距离，这里的 </p><p><img src="https://raw.githubusercontent.com/zy-Pioneer/BlogImage/main/img/2025/01/6f883bb9.png"></p>]]></content>
    
    
    <categories>
      
      <category>论文笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Incorporating Gradients to Rules_ Towards Lightweight, Adaptive Provenance-based Intrusion Detection</title>
    <link href="/2025/01/14/CAPTAIN%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <url>/2025/01/14/CAPTAIN%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<p>本文提出了一种名为CAPTAIN的基于规则的溯源入侵检测系统，该系统旨在解决现有PIDS面临的高误报率和难以适应不同环境等问题。</p><h2 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h2><h3 id="“Grey”-Nodes"><a href="#“Grey”-Nodes" class="headerlink" title="“Grey” Nodes"></a>“<strong>Grey” Nodes</strong></h3><p>随着云计算的普及，很多合法的网络连接和云服务（如AWS Lambda、Cloudflare Workers）被广泛使用，但攻击者也可以利用这些服务作为其命令与控制（C2）服务器的中介。这使得现有的基于规则的PIDS难以区分合法与恶意的外部IP连接。传统系统通常会根据简单的信任度规则对节点进行分类，但过于粗糙的规则往往会导致误报或漏报。</p><ul><li><strong>解决方案动机：</strong> CAPTAIN通过引入更细粒度的标签（如将节点信任度设置为0到1之间的浮点数），能够更灵活地反映不同程度的信任，从而避免误报或漏报。</li></ul><h3 id="依赖爆炸问题-Dependency-Explosion"><a href="#依赖爆炸问题-Dependency-Explosion" class="headerlink" title="依赖爆炸问题 (Dependency Explosion)"></a><strong>依赖爆炸问题 (Dependency Explosion)</strong></h3><p>依赖爆炸是指单个可疑事件可能导致大量系统实体被标记为可疑，尤其是在复杂的依赖链中。一个恶意事件可能引发整个依赖链上的实体被怀疑为恶意，而很多情况下这些实体其实是正常的。现有的解决方案大多采用简单的事件白名单或手动划分攻击阶段，但这些方法往往需要大量的应用程序&#x2F;操作系统的调试，无法在实际系统中大规模使用。</p><ul><li><strong>解决方案动机：</strong> CAPTAIN通过引入可调节的标签传播速率（Tag Propagation Rate），允许根据事件的类型和上下文来调整恶意标签的传播力度，从而防止不必要的传播引发依赖爆炸。</li></ul><h3 id="自定义警报触发问题-Customized-Alarm-Triggering"><a href="#自定义警报触发问题-Customized-Alarm-Triggering" class="headerlink" title="自定义警报触发问题 (Customized Alarm Triggering)"></a><strong>自定义警报触发问题 (Customized Alarm Triggering)</strong></h3><p>现有的基于规则的PIDS在触发警报时，通常对所有事件应用统一的规则和阈值。然而，在不同的情境下，同一种行为可能是正常的或恶意的。例如，<code>sshd</code>进程执行脚本是常见的行为，而<code>firefox</code>进程执行脚本则是异常的。传统的规则系统无法为不同的事件设定不同的阈值，从而容易导致漏报或误报。</p><ul><li><strong>解决方案动机：</strong> CAPTAIN引入了自适应警报触发机制，允许为不同的事件和进程设置特定的警报触发阈值，以区分正常行为和异常行为。通过这种自定义的阈值，CAPTAIN能够减少不必要的警报，并提高对异常行为的敏感性。</li></ul><h2 id="CAPTAIN-系统设计"><a href="#CAPTAIN-系统设计" class="headerlink" title="CAPTAIN 系统设计"></a>CAPTAIN 系统设计</h2><p><img src="https://cdn.nlark.com/yuque/0/2024/png/46744832/1728365003605-d50496fe-3e23-4efe-a5f2-69526c0e18a8.png"></p><p>CAPTAIN系统的设计主要分为两个部分：</p><ul><li><strong>检测模块（Detection Module）</strong>：负责处理审计日志，生成溯源图并进行实时入侵检测。</li><li><strong>学习模块（Learning Module）</strong>：在训练阶段对检测模块进行优化，通过梯度下降算法自动调整自适应参数，以减少误报并提高检测精度。</li></ul><h3 id="检测模块的设计"><a href="#检测模块的设计" class="headerlink" title="检测模块的设计"></a><strong>检测模块的设计</strong></h3><p>检测模块的核心功能是通过标签传播对系统实体（进程、文件、网络连接等）进行实时监控和检测。该模块分为以下三个主要组件：</p><h4 id="标签初始化（Tag-Initialization）"><a href="#标签初始化（Tag-Initialization）" class="headerlink" title="标签初始化（Tag Initialization）"></a><strong>标签初始化（Tag Initialization）</strong></h4><p>CAPTAIN为每个系统实体分配初始的安全标签。具体来说：</p><ul><li>**数据标签 (Data Tags)**：表示系统实体的机密性和完整性。</li><li>**代码标签 (Code Tags)**：仅用于进程，表示进程代码的可信度。</li></ul><p>系统实体首次出现在溯源图时，会根据其特征（如进程名、文件路径、IP地址等）分配初始标签。标签初始化的具体规则由自适应参数 <strong>A</strong> 决定，而该参数会在训练阶段根据反馈进行调整。</p><h4 id="标签传播（Tag-Propagation）"><a href="#标签传播（Tag-Propagation）" class="headerlink" title="标签传播（Tag Propagation）"></a><strong>标签传播（Tag Propagation）</strong></h4><p>当系统中发生事件时，CAPTAIN会根据预定义的规则在节点之间传播标签。传播的方式有两种：</p><ul><li><strong>数据流传播</strong>：数据从源节点传播到目标节点（如进程读取文件时，文件的标签会影响进程的标签）。</li><li><strong>控制流传播</strong>：进程间的交互也会传播标签（如进程执行另一个进程时，代码的可信度会传播）。</li></ul><p>标签传播的强度由 <strong>传播速率参数 (G)</strong> 控制。CAPTAIN可以为每条边设置不同的传播速率，以防止标签不必要地过度传播，特别是在应对“依赖爆炸”问题时，传播速率的自适应调整尤为关键。</p><h4 id="警报生成（Alarm-Generation）"><a href="#警报生成（Alarm-Generation）" class="headerlink" title="警报生成（Alarm Generation）"></a><strong>警报生成（Alarm Generation）</strong></h4><p>CAPTAIN系统会根据标签的变化触发安全警报。当标签传播到某个节点，并且该节点的标签值超过了设定的阈值时，就会触发相应的警报。警报触发规则由 <strong>阈值参数 (T)</strong> 控制，可以根据不同的事件类型和实体特征调整触发阈值。</p><p>例如：</p><ul><li>当进程试图执行低完整性的代码时，系统可能会触发代码完整性相关的警报。</li><li>当进程访问高机密性的数据时，系统会触发数据泄漏警报。</li></ul><p>CAPTAIN能够为不同类型的系统事件设置灵活的阈值，从而减少误报，同时确保检测到真正的威胁。</p><h3 id="学习模块的设计"><a href="#学习模块的设计" class="headerlink" title="学习模块的设计"></a><strong>学习模块的设计</strong></h3><p>CAPTAIN的学习模块负责在训练阶段优化标签传播和警报触发的参数。通过对误报数据进行分析，该模块使用<strong>梯度下降算法</strong>来调整自适应参数（A, G, T），从而改进检测模块的性能。学习模块的主要功能包括：</p><h4 id="损失函数的定义（Loss-Function）"><a href="#损失函数的定义（Loss-Function）" class="headerlink" title="损失函数的定义（Loss Function）"></a><strong>损失函数的定义（Loss Function）</strong></h4><p>学习模块使用一个损失函数来衡量系统的检测效果。损失函数由两部分组成：</p><ul><li><strong>误报项</strong>：用于惩罚错误触发的警报。如果某个事件不应触发警报但触发了，该项会增加损失值。</li><li><strong>正则化项</strong>：用于避免系统过度宽松，确保系统对恶意活动仍然保持敏感。通过将当前参数与默认初始值之间的差距最小化，CAPTAIN能够在减少误报的同时保持对攻击的敏感性。</li></ul><h4 id="梯度计算（Gradient-Calculation）"><a href="#梯度计算（Gradient-Calculation）" class="headerlink" title="梯度计算（Gradient Calculation）"></a><strong>梯度计算（Gradient Calculation）</strong></h4><p>CAPTAIN将检测过程建模为可微分的函数，以便在训练过程中计算自适应参数的梯度。通过这些梯度，系统能够使用梯度下降算法不断更新参数，从而优化标签传播和警报生成规则。</p><p>例如：</p><ul><li>对于节点的初始标签（A）的梯度，系统根据标签如何传播来调整初始值，以减少误报。</li><li>对于边的传播速率（G），系统通过传播链上各节点的行为来计算传播速率的梯度，从而控制信息在系统中的传播程度。</li><li>对于警报阈值（T），梯度用于调整系统对不同事件的敏感性，避免过多或过少的警报触发。</li></ul><h4 id="训练与测试流程"><a href="#训练与测试流程" class="headerlink" title="训练与测试流程"></a><strong>训练与测试流程</strong></h4><p>在训练阶段，CAPTAIN会基于良性数据进行训练。训练的结果是生成一个优化的自适应参数配置。在测试阶段，系统根据训练得出的最优参数对新的系统事件进行实时检测。</p><h3 id=""><a href="#" class="headerlink" title=""></a></h3>]]></content>
    
    
    <categories>
      
      <category>论文笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>安全</tag>
      
      <tag>入侵检测</tag>
      
      <tag>PIDS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>神经网络调参技巧</title>
    <link href="/2024/11/14/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%B0%83%E5%8F%82%E6%96%B9%E6%B3%95/"/>
    <url>/2024/11/14/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%B0%83%E5%8F%82%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="调参顺序"><a href="#调参顺序" class="headerlink" title="调参顺序"></a>调参顺序</h2><p>第一个要进行调整的参数就是学习率，需要设置一个好的初始学习率，它可以决定损失函数的降低速度以及损失函数的最低位置。通常从 0.01 开始，**范围从 [1e-6,1e-1]**，通常只选择 1e<sup>n</sup> 。</p><p>学习率优化器是针对每个参数进行调整的，学习率调度器是对全局的学习率进行调整的。</p><table><thead><tr><th><strong>学习率调度器</strong></th><th><strong>学习率优化器</strong></th><th><strong>组合优势</strong></th><th><strong>适用场景</strong></th></tr></thead><tbody><tr><td><strong>StepLR</strong></td><td>SGD, Adam</td><td>定期减小学习率，简单高效，防止过拟合</td><td>大多数监督学习任务，如分类、回归任务</td></tr><tr><td><strong>ReduceLROnPlateau</strong></td><td>SGD, Adam</td><td>自适应调节学习率，基于验证损失自动减少，避免学习率过小</td><td>验证集性能停滞的任务，如回归、复杂的模型训练</td></tr><tr><td><strong>ExponentialLR</strong></td><td>SGD, Adam, RMSprop</td><td>指数衰减学习率，快速收敛，减少训练时间</td><td>快速收敛场景，适合较大规模数据集</td></tr><tr><td><strong>CosineAnnealingLR</strong></td><td>SGD, Adam</td><td>平滑衰减学习率，避免震荡，适合长时间训练</td><td>长期训练的大型深度神经网络，如卷积神经网络</td></tr><tr><td><strong>CyclicLR</strong></td><td>Adam, RMSprop</td><td>周期性调整学习率，跳出局部最优，稳定优化</td><td>复杂优化任务，如生成对抗网络（GAN）、深度强化学习</td></tr><tr><td><strong>OneCycleLR</strong></td><td>Adam, SGD</td><td>先增大学习率，后逐渐减小，快速找到最优解</td><td>快速训练大规模神经网络，如图像分类、NLP大模型</td></tr><tr><td><strong>LambdaLR</strong></td><td>任意优化器</td><td>完全自定义学习率变化策略，灵活可控</td><td>特定需求场景，无法用其他调度器满足的情况</td></tr><tr><td><strong>MultiStepLR</strong></td><td>SGD, Adam</td><td>在特定里程碑时减少学习率，适合多阶段训练</td><td>长期训练的任务，如卷积神经网络、阶段性改进模型性能</td></tr></tbody></table><h2 id="过拟合缓解措施"><a href="#过拟合缓解措施" class="headerlink" title="过拟合缓解措施"></a>过拟合缓解措施</h2><table><thead><tr><th><strong>方法</strong></th><th><strong>原理</strong></th><th><strong>适用场景</strong></th><th><strong>优势</strong></th><th><strong>使用场景</strong></th></tr></thead><tbody><tr><td><strong>L2 正则化</strong></td><td>在损失函数中加入权重的平方和作为惩罚项，防止权重过大</td><td>大多数神经网络，尤其是复杂的深度模型</td><td>减少权重过大，提升模型的泛化能力</td><td>数据量大、模型复杂，可能出现过拟合的场景</td></tr><tr><td><strong>L1 正则化</strong></td><td>在损失函数中加入权重的绝对值和作为惩罚项，倾向于使部分权重趋近于零</td><td>稀疏模型、特征选择</td><td>实现特征选择，简化模型</td><td>需要稀疏性或特征选择的任务，如文本分类</td></tr><tr><td><strong>Dropout</strong></td><td>训练时随机丢弃部分神经元，减少神经元间共适应性</td><td>深度神经网络，尤其是全连接层较多的模型</td><td>提升泛化能力，减少共适应性</td><td>卷积神经网络（CNN）、循环神经网络（RNN）</td></tr><tr><td><strong>早停法（Early Stopping）</strong></td><td>监控验证集性能，当性能不再提升时停止训练</td><td>所有深度学习任务，特别是长时间训练的任务</td><td>节省训练时间，防止过拟合</td><td>长时间训练的任务，如大型神经网络、语言模型</td></tr><tr><td><strong>数据增强</strong></td><td>通过随机变换训练数据（如翻转、裁剪等）增加数据的多样性</td><td>图像分类、时间序列处理等</td><td>增强数据集多样性，提高模型泛化能力</td><td>图像分类（翻转、旋转）、语音处理（噪声增强）</td></tr><tr><td><strong>Batch Normalization</strong></td><td>对每层的激活值进行标准化处理，使输入具有稳定的分布</td><td>深层神经网络，如卷积神经网络和循环神经网络</td><td>加速收敛，防止过拟合，训练更稳定</td><td>深层模型（如ResNet、VGG等），用于加速训练和稳定</td></tr><tr><td><strong>减少模型复杂度</strong></td><td>减少模型的参数量（如降低网络层数、神经元数）</td><td>小数据集、简单任务</td><td>通过简化模型结构减少过拟合风险</td><td>数据较少、简单任务，如小型图像分类任务</td></tr><tr><td><strong>交叉验证</strong></td><td>将数据划分为多个子集，循环验证模型性能</td><td>小数据集、模型调参</td><td>多次训练提高模型评估的稳定性</td><td>小数据集，模型调优，避免单次训练评估误差的场景</td></tr><tr><td><strong>添加噪声</strong></td><td>在输入、隐藏层或输出层添加噪声，增加模型的训练难度</td><td>时间序列、音频处理任务</td><td>增强模型鲁棒性，防止过拟合</td><td>语音、时间序列等，模型需要适应噪声的场景</td></tr><tr><td><strong>调节批次大小</strong></td><td>使用较小的批次大小，增加训练中的随机性</td><td>深度学习模型，特别是CNN和RNN模型</td><td>小批次能有效提升模型的泛化能力</td><td>大规模数据处理，小批次可能提升泛化效果的场景</td></tr><tr><td><strong>正则化项</strong></td><td>在损失函数中添加特定任务的自定义正则化项</td><td>特定任务，如有特定约束的模型</td><td>更好地控制模型的复杂性</td><td>有明确的任务约束，如控制输出范围的场景</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>技术分享</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
